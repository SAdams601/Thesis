% Documentation:
%
% The UoK class file extends the standard report style to follow the Registry
% guidelines for laying out a thesis. It sets the margins, interline spacing,
% the page, figure and table numbering style, and disallows page breaks at
% hyphens. The class file consists of setting one and an half line spacing text
% with a 4cm left margin, at least a 2.5cm right margin, approximately 2cm top
% and bottom margin, on A4 paper.
% 
% The class the following options, in addition to those of the standard report
% class.
%     mini - Toggles the thesis in to mini-thesis mode. This adds "mini" to the
%            title and appends a nocite(*) at the end for an automatic output of
%            your complete bibliography.
%     draftmark - Puts a DRAFT' watermark on every page of the document along
%                 with the draft statement on the title page. Additionaly, it
%                 is used as a switch for the UoKExtentions package.
%     draft - Puts the entire document into draft mode. Applies all the effect
%             of draftmark above, but also propergates to other packages used.
%     copyright - Adds a copyright page between the title page and the preface.
%     nofig - Disables output of the list of figures in the preface.
%     notab - Disables output of the list of tables in the preface.
% All options passed to UoKthesis will be passed along to included packages:
%    natbib, draftwatermark, setspace, hyperref, lmodern
%
% The cover page and optional copyright page are implicitly added before the
% start of the preface section. Use the following commands to populate the 
% cover page/copyright page information:
%     \title{thesis title}
%     \author{author's name} 
%     \degree{Master of Science, Doctor of Philosophy, etc.} 
%     \subject{author's department}
%          - Computer Science if omitted 
%     \submitdate{month year in which submitted}
%          - dated by LaTeX if omitted 
%     \copyrightyear{year degree conferred (next year if submitted in Dec.)}
%          - assumes current year (or next year, in December) if omitted 
% 
% The preface environment allows for the use of sections that precede the main
% document; such as Abstract and  Acknowlegements. These sections should be
% defined using \section{Preface Section Title}. The contents page (and list of
% figures and tables if in use) will be automatically inserted at the end of the
% preface environment.
%
% The thesis style invokes the setspace package to set the commands:
%     \doublespace
%     \onehalfspace
%     \singlespace
% for spacing. By default one and an half spacing is used which resembles the
% UKC Typewriter requirement. Singlespace can be used for letterpress
% appearance. If you want to use true double space, for some reason, place the
% \doublespace command where you want to start using double spacing. Just call
% the appropriate spacing command at where you want to use them.
% 
% In the figure and table environments, single spacing is used. If you want to
% use any other size rather than one and an half spacing, then do:
% 	\renewcommand{\baselinestretch}{1.6} (or whatever you want instead of 1.6)
% This command won't take effect unless it comes before the \begin{document} or
% is triggered by a font change (after something like \small \normalsize).
%
% The example below shows the 12pt thesis style being used. This seems to give
% acceptable looking results, but it may be omitted to get 10pt. Alternatively,
% the 11pt option can be used.
%
% This version differs from old_ukcthesis.sty in the following ways:
% 1. Removed the doublespace package (now uses setspace).
% 2. Merged the phantom section for correct PDF links into the bibliography
%    generating function. 
% 3. Added thesis type options (mini, draft).
% 4. Kent Harvard is used for referencing and citation, this is supported by the
%    natbib package.
% 5. PsFig macro removed.
% 6. Now comes as two files, UoKthesis.cls, which defines purely stylistic layout,
%    and UoKextentions.sty, that provideds some additional functionality.

\documentclass[12pt]{UoKthesis}

%\renewcommand*\rmdefault{ptm}
%\renewcommand{\familydefault}{\rmdefault}
% Note: The UoKextentions package includes the xcolor package with the [usenames]
% options. If you need to add further options, these can be given to UoKextentions
% to be propogated through.
\usepackage{UoKextentions}
\usepackage{times}
%\usepackage{llncsdoc}
%\usepackage{verbatim}
\usepackage{url}
\usepackage{color}

\usepackage{amsmath}
\usepackage{relsize}
\usepackage[final]{listings}
\usepackage[T1]{fontenc}
%\usepackage[math]{times}
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\newcommand{\td}[1]{{\bf {\tt{#1}}}}
\newcommand{\comment}[1]{\textcolor{red}{\td{{#1}}}}
\usepackage{textcomp}
\usepackage{csquotes}
\lstset{
  frame=none,
  xleftmargin=2pt,
  stepnumber=1,
  numbers=left,
  numbersep=5pt,
  numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  stringstyle=\mdseries\ttfamily,
  showspaces=false,
  keywordstyle=\bfseries\ttfamily,
  columns=flexible,
  upquote=true,
  showstringspaces=false,
  basicstyle=\small\ttfamily,
  breaklines=true,
  morecomment=[l]\%,
}

% Kent Harvard Bibliography Style. WIP
\bibliographystyle{kentHarvard}

% Provides nice linking in PDFs
\usepackage{hyperref}

% Only needed if you want to produce an index. Example is shown at the bottom of this document.
\usepackage{makeidx}

% Useful packages
% \usepackage{epstopdf} % Converts EPS files to PDF using ghostscript
% \usepackage{fnbreak}  % Warns you if you have split footnotes
% \usepackage{mathpazo} % Type­set­ math­e­mat­ics in the Palatino fam­ily of text fonts
% \usepackage{paralist} % Enumerate and itemize within paragraphs
% \usepackage{amsmath}  % AMS mathematical facilities
% \usepackage{rotating} % Rotating facilities for floats


\setcounter{secnumdepth}{3} % add more section types

%%%%% macros
\def\fixme#1{\fbox{\textbf{\textsc{Fixme}}\quad#1}}
\def\fixpic#1{\fbox{\textbf{\textsc{Picture}}\quad#1}}
\def\defnx#1#2{\emph{#1}\index{#2}}
\def\defn#1{\defnx{#1}{#1}}
\def\floatpic#1#2{%
\begin{wrapfigure}{r}{\dimexpr #1 / 2 \relax}
\includegraphics[width=\dimexpr #1 / 2 \relax]{#2}
\end{wrapfigure}}
\def\inlinepic#1#2{%
\begin{center}
\includegraphics[width=\dimexpr #1 / 2 \relax]{#2}
\end{center}}

%%%%% augment hyphenation
\hyphenation{wide-spread}

%%%%% document start
\begin{document}

\title{Data-Driven Refactorings for Haskell}
\author{Stephen Adams}
\subject{Computer Science}
\degree{PhD}

\begin{preface}
\section{Abstract}

Refactoring is the process of changing the internal structure of a program without changing its external behaviour. Refactoring increases code quality and reduces its technical debt. However, refactoring by hand is time consuming and error-prone. This makes automated refactoring tools very useful.

Agile software development allows for software to evolve slowly over time. This evolution changes how a program processes, abstracts over, and views its data. This evolution, though necessary, comes with the cost of technical debt. As technical debt increases changes to a code base become more difficult. Refactoring is one of the primary ways to  reduce technical debt. 

There exist refactorings that specifically help software to evolve its data model, however these refactorings are specific to the object-oriented programming paradigm. Haskell is a strongly typed, pure functional programming language. Haskell's rich type system allows for complex and powerful data models and abstractions. This thesis reports on work done to design and automate refactorings that help Haskell programmers develop these abstractions.

This work also discussed the current design and implementation of HaRe (the \textit{Ha}skell \textit{Re}factorer). HaRe now supports the Glasgow Haskell Compiler's implementation of the Haskell 2010 standard and its extensions. 

\section{Acknowledgements}
I would like to thank...
\end{preface}

\chapter{Introduction}\label{chp:intro}


\section{Functional Programming}
Functional programming is a programming paradigm that focuses on data values as described by expressions which are built from function applications and definitions~\citep{elementsOfFunc}.  Functions in this case are closely related to the idea of mathematical functions. Another key concept of a functional programming language is that functions are \textit{first class citizens}. This means that functions can be used like any other type of data, e.g. as arguments to other functions. 
  
\section{Haskell}
Haskell is a statically typed, lazily evaluated, pure functional language. Haskell is strongly and statically typed, and supports Hindley-Milner type inference~\citep{wikiIntro}. Type inference means that a Haskell programs do not need every type to be explicitly listed in the source code. Types will be~\textit{inferred} at compilation time so that every part of a Haskell program's type is known at that time. Haskell's type system also allows users to define their own types.

Lazy evaluation means that Haskell expressions are not evaluated when they are passes as a parameter, but rather when that value is used. For example in the Haskell function:
\clearpage
\begin{verbatim}
f x y = case x > 0 of
True -> x - 1
False -> x + 1
\end{verbatim}

The parameter y will never be evaluated.

Haskell is also a pure language. Purity is the idea that functions cannot perform actions in addition to returning values. These additional actions are known as side-effects. Haskell allows for traditionally side-effect causing operations (IO, state, etc.) through the use of monads. Monads contain any potentially side effect causing operations inside of them allowing the rest of the program to remain pure.

\section{Refactoring} 
Refactoring is the process of changing a program without changing its behaviour. This is done to improve its internal structure~\citep{fowler}. Behaviour preservation is what separates refactoring from other types of program manipulation. This idea of functionality preservation means that refactoring will not introduce new bugs or eliminate old ones. To prevent semantic changes after refactoring, many refactorings have non-trivial preconditions~\citep{refacTools}. 

Manual refactoring is tedious and error prone, and depends on high testing coverage to ensure that functionality is preserved~\citep{fowler}. This means that tools that can automatically perform refactorings and ensure that preconditions are met are highly desirable.

\subsection{Functional refactoring}

Refactoring a functional language has a few key differences from refactoring an imperative language. The higher-order nature functional languages means that any sub-expression of a function is a candidate for generalisation whereas in other languages the types of parameters and results is limited. The semantics of functional languages also allow for the complete checking of preconditions based on the static semantics of the language~\citep{refacTools}.

It is also not unusual for functional refactorings to be substantially different than their object-oriented (OO) counterparts. For example creating a case statement from a multi-equation function definition in a functional language versus inlining a virtual method as a case statement in an OO language require substantially different program manipulations~\citep{huiqingThesis}. Additionally there can be refactorings with no OO counterpart, monadification, for instance.  

\section{Contributions of this Research}

\comment{TODO}

\section{Justification and limitations of tooling}

A refactoring tool can only do so much the user needs to be "sufficiently smart" \comment{elaborate on this}

For tools to be successful they just have to bridge the gap between the original code to a version of the code that the programmer is willing to modify to what they want. Without the tool the transformation would be too risky or monotonous for the user to attempt alone.

  

\section{Thesis Outline}

\comment{TODO}


\chapter{Background: Refactoring Haskell in HaRe}
\label{hare}

\comment{
Chapter~\ref{hare} is where the development and implementation of HaRe will be discussed. The chapter with cover some of the history or HaRe and the briefly the technology that it was originally developed with. Next it will cover the design and implementation of HaRe currently and its dependencies (in particular ghc-exactprint). }

\td{
A brief outline of this chapter:

\begin{enumerate}
\item The original implementation of HaRe
\item The GHC API
\item Generic Programming (Scrap Your Boilerplate and Strafunski-StrategyLib)
\item GHC-ExactPrint
\item The Current version of HaRe
\end{enumerate}
}

Work on HaRe started in early 2003 at the University of Kent. HaRe was originally created by Huiqing Li, Claus Reinke, and Simon Thompson~\citep{refacWebsite}. This chapter begins with a brief discussion of the original implementation of HaRe. HaRe was originally implemented to support the Haskell 98 standard~\citep{huiqingThesis}. The Haskell ecosystem has evolved a great deal since then. Haskell 2010 is the current formal language standard but the Glasgow Haskell Compiler (GHC) has become the de facto Haskell standard~\citep{refacTools}. GHC supports the entire 2010 standard but also includes language extensions that do everything from changing the type system to adding new syntax features~\citep{langExts}.



\section{The original implementation of HaRe}

Implementing an automated refactoring system has several dependencies, a frontend for the language that you are targeting, a generic programming library, and a pretty printer. The language frontend is required for the refactorer to analyse and modify source code, the generic programming library assists in traversing the complex abstract syntax tree of a real world programming language, and the pretty printer outputs the modified AST in a form recognizable by the author of the original program. The original implementation of HaRe fulfilled these dependencies with two libraries, Programatica and Strafunski-StrategyLib. 

\subsection{Programatica and Strafunski}\label{prog&Strafunski}

Programatica was a project at the OGI School of Science and Engineering to build tool support for validating Haskell programs~\citep{programaticaTools}. The Programatica team open sourced their frontend so that other tools could also use it~\citep{refacWebsite}. The HaRe team chose to use Programatica over other available front ends (such as GHC's internal front end) because it was the simplest front end that supported the full Haskell 98 standard along with a number of its extensions~\citep{huiqingThesis}. Programatica's Haskell front end is broken up into multiple components including a lexer, a parser, an abstract syntax, a module system, a type checker, and a pretty printer. Programatica's frontend allows for HaRe to focus on refactoring only rather than having to build all of these components as well.

Programatica's abstract syntax contains 20 data types with 110 data constructors in total. Working with the syntax directly would introduce a large amount of "boilerplate" code into HaRe that would make maintenance and reusability of much more difficult~\citep{huiqingThesis}. Instead HaRe used Strafunski-StrategyLib, a combinator library for generic programming, to traverse the abstract syntax tree (AST) of the source code~\citep{strafunski}. 

Commonly transforming a program only modifies small sections of the source program's AST. Renaming a function, for example, will only need to modify the name used in the binding and places where that name is being used; all other sections of the AST remain unmodified. A commonly used operation in Strafunski takes a function that works on a particular data type (or types)\footnote{In the renaming example this could be code that checks if a variable is the one being renamed and replacing it with the new name.} and extends it to work on all types by leaving all other types unmodified. Strafunski also provides "strategies" that define how that extended function will be applied to the syntax tree~\citep{strafunski}\footnote{e.g. top-down or bottom-up}. The using Strafunski will be discussed in more detail in section~\ref{genProg}.

These two dependencies allowed the original implementation of HaRe to obtain the abstract syntax tree of source code to be refactored, traverse and transform that AST, and output the modified program. All these tasks are things that the current implementation of HaRe needs to do but the dependencies that it relies on have changed somewhat. The Haskell standard was updated in 2010~\citep{haskell2010} and GHC continues to expand the number of language extensions it supports. Unfortunately Programatica has not kept up with these changes and does not support anything beyond Haskell 98. At the same time GHC's own API has matured so that it can replace Programatica as HaRe's front end. HaRe now relies on this and a few other projects for its language frontend. For generic traversals "Scrap Your Boilerplate" has been added as a dependency though Strafunski-StrategyLib is still used~\citep{syb}.
 
\subsection{HaRe's original refactorings}\label{origRefactorings}

HaRe's original refactorings fall into three categories, structural, module, and data-oriented refactorings.

\subsubsection{Structural Refactorings}

Structural refactorings principally concern the name and scope of entities defined in a program~\citep{huiqingThesis}. These refactorings modify functions, and smaller sections of code. A traditional example of this is the renaming refactoring. 

Renaming is one of the most basic refactorings. The purpose of renaming is to change the name of a given identifier. A renaming refactoring could target a variable, function name, type, or any other piece of syntax that a programmer can name. This allows for the way in which you refer to your code to easily stay up to date with what your code actually does. 

Other examples of structural refactorings include deleting a definition, duplicating a function, and adding an argument~\citep{huiqingThesis}.

\subsubsection{Module Refactorings}

Module refactorings concern the imports and exports of an individual module, or the relocation of definitions between modules~\citep{huiqingThesis}. A simple refactoring in this category would be "clean an import list," which analyses a module's import list and removes redundant import declarations. Another example of a refactoring in this category would be the "move a definition" refactoring. As its name suggests, move a definition takes a definition from one module and moves it to another and fixes the imports and exports of any affected modules. For example if we were moving some function \texttt{foo} from module \texttt{A} to module \texttt{B} any external dependencies of \texttt{foo} need to be imported into \texttt{B} and if those dependencies are no longer used in \texttt{A} the import relevant statements should be removed. If \texttt{foo} is still being used in \texttt{A} then \texttt{A} needs to import \texttt{B} (if it does not already do so). Finally any other modules that currently depend on \texttt{foo} need to now import \texttt{B} (if it's not already imported) and remove the import of \texttt{A} (assuming none of \texttt{A}'s other definitions are used).

\subsubsection{Data Type Based Refactorings}

The third category of refactorings are those that are associated with data type definitions~\citep{huiqingThesis}. "Add field names" is a good example of a data-oriented refactoring.  The add field names refactoring will add field names to a data type. These names can then be used as selector functions that make extraction of a particular part of a type a simple function call. The new field names are generated by HaRe but can be renamed by the user~\citep{huiqingThesis}.

The adding of field names alone seems like an unusual 
\comment{Add description of concrete to abstract data type refactoring. Also use it to introduce composite refactoring. See: https://www.cs.kent.ac.uk/projects/refactor-fp/catalogue/ConcreteToAbstract.html and huiqings thesis}

These original refactorings were chosen to be basic yet still useful, and for their ability to give insight into the issues surrounding implementing an automated refactoring tool~\citep{huiqingThesis}. In addition to the refactorings that were implemented by HaRe's developers an API was exposed so that other developers could implement their own refactorings~\citep{hareApi}.


\subsection{The HaRe API}\label{hareApi}

Early in the development cycle of HaRe it was restructured to expose an API for implementing refactorings and general Haskell program transformations~\citep{hareApi}. The HaRe API contains a collection of functions for program analysis and transformation of Haskell 98 programs. These functions along with the functionality provided by Strafunski and Programatica form the basis for implementing basic refactorings~\citep{hareApi}.

The HaRe API exposes the full Programatica abstract syntax for Haskell 98 to the user but because of generic programming with Strafunski only the to be transformed parts of the AST have to be explicitly referenced in a refactoring~\citep{hareApi}. Another key feature of the API was to hide layout and comment preservation allowing the programmer to focus on program transformation instead. Each subtree in the AST is tagged with its absolute location in the source file. Any modifications to the AST will change the location of all elements that occur after the change. The API abstracts over this cascade of changes that follows even the simplest of changes. The HaRe API transformation functions modify the token stream and the AST simultaneously which keeps refactoring definitions free of this location bookkeeping ~\citep{hareApi}. 

The overall goal of the HaRe API is to help ensure the correctness of new refactorings by limiting the amount of code required that is not related to program transformation, and isolating common error sources~\citep{hareApi}. This design goal still guides the development of HaRe, and many of the functions that were provided in the original HaRe API have been updated to HaRe's latest implementation. 
 
\section{Underlying technologies}

As was mentioned in Section~\ref{prog&Strafunski}, HaRe's dependencies have changed somewhat in its current implementation. HaRe originally used Programatica as its front end, now the language front end is composed of several projects now, the GHC API, ghc-mod, and ghc-exactprint~\citep{ghcApi}, \citep{ghcMod}, and \citep{exactprint}. The following sections will describe these dependencies as well as Scrap Your Boilerplate~\citep{syb}, another generic programming library, and how HaRe currently uses them.

\section{The GHC API}

Rather than being a monolithic executable, the Glasgow Haskell Compiler (GHC) is composed of several smaller components that each correspond to a separate compiler stage. GHC's executable consists of a lightweight main function that ties together the smaller components~\citep{ghcDesign}. These components are exposed to users  and this is what constitutes the GHC API.

\subsection{Compiler stages of GHC}\label{ghcStages}

\begin{figure}[h]\label{compilerStages}
	\begin{center}
		\includegraphics[scale=.4]{graphVis/Chapter2/compilerStgs.png}
	\end{center}
	\caption{GHC Compiler stages.}
\end{figure}

Some of the major components and the order that GHC uses them are shown in figure~\ref{compilerStages} which has been adapted from~\citep{ghcDesign}. This figure is not a complete list of all the components GHC uses just the parts that HaRe interacts with. The full diagram can be found in~\citep{ghcDesign}. The label after each compiler stage indicates the type of AST that is produced by that stage.

The top level datatype for all of the GHC abstract syntax is \texttt{HsSyn} ~\citep{ghcDesign}. \texttt{HsSyn} is parameterised by some identifier type; each compiler stage produces a different type of identifier with the additional information that stage produces. For example the typechecker takes in an AST parameterised by \texttt{Name} and returns an AST parameterised by \texttt{Id} which is a \texttt{Name} with additional type information.

\subsection{GHC's Name Types}\label{ghcNames}

There are five name types that GHC uses, they are:

\begin{itemize}
	\item \texttt{OccName} is the simplest type of name. It is just a wrapper around a \texttt{FastString} and an optional \texttt{NameSpace}. An \texttt{OccName} is contained in each of the other four identifier types.
	\item \texttt{RdrName} names are produced by GHC's parser. \texttt{RdrName}s aren't much more that an \texttt{OccName} with optional module information if the source name has been qualified. 
	\item \texttt{Name}s are produced by the renamer. A \texttt{Name} contains an \texttt{OccName} and a \texttt{Unique} that differentiates \texttt{Name}s that have the \texttt{OccName}. They have also had their scoping and binding resolved.
	\item \texttt{Id} \& \texttt{Var} are the identifiers produced by the typechecker.\footnote{\texttt{Id} is just a synonym for \texttt{Var}.} These identifiers both contain a \texttt{Name}, a \texttt{Unique}, and a \texttt{Type} or \texttt{Kind}. An \texttt{Id} has a definite type whereas a \texttt{Var} may contain type variables that only have a kind.   
\end{itemize}

These identifier types change throughout the compilation process but these names only parameterise the syntax tree, the shape of the tree itself stays the same through compilation. 

\subsection{GHC's syntax tree}
GHC's abstract syntax is currently made up of over 90 data types. Many of those types has multiple constructors, the expression data type \texttt{HsExpr} for example has over 40 constructors. This section will briefly introduce the structure of GHC's abstract syntax tree.

\subsubsection{Common syntax types}

The most common type in any given piece of GHC abstract syntax would be a \texttt{Located} as seen in listing~\ref{located}.

\begin{lstlisting}[caption={The \texttt{located} type.},label=located]

type Located e = GenLocated SrcSpan e

\end{lstlisting}

\texttt{Located} is used to tag syntax elements with their original position in a source file. The \texttt{SrcSpan} contains the filename that the span comes from and then the start and end columns and start and end lines that the span covers.\footnote{GHC has small optimisation where if a span exists entirely on a single line it only stores the single line number and the start and end column, instead of storing the same line number twice.} 

There are located versions of many of the AST types. For example, \texttt{HsExpr} is the type that represents expressions there is a related type \texttt{LHsExpr} that represents a located expression.

\begin{lstlisting}[caption={The located expression}]
type LHsExpr id = Located (HsExpr id)
\end{lstlisting}

The \texttt{HsExpr} type represents much of the Haskell language such as function application, lambdas, if and case statements. Pattern matching is handled by a \texttt{MatchGroup} type. Each \texttt{MatchGroup} contains a list of \texttt{Match}es and some typing information. Consider the following case statement

\begin{lstlisting}[caption={A case statement}, label=caseStmt]
case (x, y) of
	(Just i, Just j) -> Just (i+j)	
	(Just _, Nothing) -> x
	(Nothing, Just _) -> y
	(Nothing, Nothing) -> Nothing
\end{lstlisting}

This would be parsed into the \texttt{HsCase} constructor of \texttt{HsExpr}. This constructor is seen in listing~\ref{hscase}, the first constructor argument represents the expression that the patterns are matched against, (the tuple \texttt{(x,y)} in this case). The second argument to \texttt{HsCase} represents all four of the matches in the case statement. \texttt{MatchGroup}s are used to represent any code that associates some patterns with a right hand side expression. Function bindings, lambda expression, and case statements all use \texttt{MatchGroup}s. 

\begin{lstlisting}[caption={The \texttt{HsCase} constructor}, label=hscase]
HsCase (LHsExpr id) (MatchGroup id (LHsExpr id))	
\end{lstlisting}

In the case expression from listing~\ref{caseStmt} each of the four pattern matches is a \texttt{Match} in the expressions \texttt{MatchGroup}. Each \texttt{Match} associates a left hand side pattern with the appropriate right hand side expression.

\subsubsection{The syntax tree}
The previous section gave a brief overview of GHC's identifiers and its representation of expressions. This section describes the broader picture, the representation of whole Haskell programs.  

According to the GHC a Haskell program is simply a list of modules.\footnote{This is a very simple view and additional tools are needed to properly represent "projects" which is the context that most Haskell programs exist inside of. This is discussed further in section~\ref{hareInners}.} Inside of GHC each module is represented by an \texttt{HsModule}. This top level structure keeps track of everything that module imports, exports, and a list of all the declarations that the module defines. Declarations (of type \texttt{HsDecl}) are what represent everything that can be defined in Haskell. 

The \texttt{HsDecl} type is only used as a wrapper around other types. Bindings (of functions and/or values), instance, and type class declaration are differentiated by \texttt{HsDecl}'s constructors. 

\begin{lstlisting}[caption={A subset of \texttt{HsDecl constructors}}]
data HsDecl id =
	   TyClD (TyClDecl id)
	| InstD (InstDecl id)
	| ValD  (HsBind id)
\end{lstlisting}

\texttt{HsDecl} is only provides a single type that describes what sort of declaration it is. There is very little information stored at this level, the inner type is the "payload" of the declaration (e.g. \texttt{HsBind} or \texttt{TyClDecl}). These payload types are what store the type and expression level abstract syntax of Haskell that was described in the previous section.


\section{Generic programming}\label{genProg}
The need for a generic programming library, as previously discussed in section~\ref{prog&Strafunski}, remains the same when using the GHC API's AST as opposed to Programatica's. Currently HaRe still uses Strafunski-StrategyLib as well as another library, Scrap Your Boilerplate. Scrap Your Boilerplate (SYB) is a generic programming library developed by Ralf L{\"a}mmel and Simon Peyton-Jones~\citep{syb}.
\subsection{Generic Traversals}
The Stratego/XT library was one of first systems for programming tree transformations in a systematic way~\citep{stratego}. Stratego developed the idea of a transformation strategy. A strategy is the combination of a term rewriting function and a traversal function that describes how that rewriting function should be applied to a tree of terms. Stratego provides combinators that help construct term rewriting functions and tree traversal functions \citep{stratego}.

Stratego is an untyped transformation system and so was unsuitable for working with the statically typed Haskell.  
\subsection{Scrap Your Boilerplate}\label{syb}

	Suppose there was a simple expression language that contained integers, integer addition, assignment, and variables. This language is represented by the type defined below.
	
	\begin{lstlisting}[caption={A simple expression type.},label=expression]
type Name = String

data Expr =
     Value Int
   | Var Name
   | Add Expr Expr
   | Assign Name Expr
      deriving(Data,Typeable)
	\end{lstlisting}
	
	A function to rename a variable "x" to "a" is defined below:
	
	\begin{lstlisting}
renameXVar :: Expr -> Expr
renameXVar (Var "x") = Var "a"
renameXVar (Assign c e) = 
	| c == "x" = Assign "a" (renameXVar e)
	| otherwise = Assign c (renameXVar e)
renameXVar (Add e1 e2) = Add (renameXVar e1) (renameXVar e2)
renameXVar v = v
	\end{lstlisting}
	
	This is fairly straightforward and doesn't take much time to write. However, if subtraction was added to the definition of expression \texttt{renameXVar} would need to be updated as well to include a recursive call very similar to the addition case. These duplicated recursive calls are what is known as "boilerplate" code~\citep{syb}. Boilerplate code is highly repetitive, verbose, and difficult to debug and maintain.  In this small example having a few of these types of cases is not an issue. However, as the expression begins to approach the size of an actual programming language writing traversals like \texttt{renameXVar} become much more time-consuming and a nightmare to maintain. 
	
	The reduction of boilerplate code like this is the point of SYB. SYB allows us to rewrite \texttt{renameXVar} as:
	
	\begin{verbatim}
import Data.Generics	
	
rename :: Name -> Name
rename "x" = "a"
rename n = n

renameXVar :: Expr -> Expr
renameXVar = everywhere (mkT rename)
	\end{verbatim} 
	
	This example nicely illustrates the four key components of an SYB traversal~\citep{syb}.

	\begin{itemize}
		\item The function that performs the "interesting" part of the traversal
		\item A type extension for that function
		\item A generic traversal combinator
		\item The data type to be traversed must be an instance of the \texttt{Typeable} and \texttt{Data} classes (as explained below)
	\end{itemize}
	
	From the example mentioned previously the "interesting" part of this traversal is the rename function because this function contains the code that actually changes the name "x" to the name "a." The \texttt{mkT} function extends the type of the \texttt{rename} function to \texttt{Typeable~a = > a~ -> a}. 
	
	Type extension allows for the \texttt{rename} function to work over any members of the \texttt{Typeable} class rather than just \texttt{Name}s. The extended version of \texttt{rename} will work as expected when provided with an argument of type \texttt{Name}, and will act as the identity function if an argument of any other type is provided.
	
	The \texttt{everywhere} function is this traversal's generic combinator, \texttt{everywhere} applies a generic function to every node in the tree. Finally as you can see in the declaration of \texttt{Expr} derives both the Typeable and Data classes so it can be traversed by \texttt{everywhere}. A member of the Typeable class has defined a generic representation of itself and members of the Data class implement generic folding operations. Put together these two classes are what allow a data type to be generically traversed. 
	
\subsubsection{Types of Generic Algorithms}

SYB defines three types of generic algorithms, transformations, queries, and monadic transformations. The \texttt{rename} example from the previous section is an example of a transformation. Transformations preserve the type of the structure that is traversed. Queries, on the other hand, are "type unifying" algorithms. Queries are good for summarizing information contained in a data structure. You would use a query, for example, to traverse an expression and collect all of it's the bound variables. Using the same expression type (listing~\ref{expression}) from the previous section the following function extracts all bound variables from a given expression.

\begin{lstlisting}[caption={A generic function that collects all bound variables from an expression.}]
bVars :: Expr -> [Name]
bVars e = everything (++) ([] `mkQ` f) e
	where f (Assign nm _) = [nm]
          f _             = []
\end{lstlisting}

\texttt{everything}, as seen in the \texttt{bVars} function from the previous listing, is the generic query combinator that summarises all nodes, top down from left to right~\citep{sybDocs}. The first argument to \texttt{everything} is the function it uses to combine separate results from the query. In this case all the lists of names will be appended together. The "interesting" function \texttt{f} that actually returns a \texttt{Name} when one is being bound is extended via the \texttt{mkQ} function. \texttt{mkQ} will apply \texttt{f} when possible otherwise it will just return the empty list.

It is also useful a lot of the time to do transformations from within some monadic context. A simple example of this would be to rewrite the finding bound variables example from previously but instead the list of found results is stored as a piece of state.

\begin{lstlisting}[caption={Finding bound variables using the state monad}]
type TransformState = State [Name]

bVars :: Expr -> [Name]
bVars e = execState findVars []
  where findVars = everywhereM (mkM f) e
        f :: Expr -> TransformState Expr
        f e@(Assign n _ ) = do
          modify (\lst -> n:lst)
          return e
        f e = return e
\end{lstlisting}

A big advantage of monadic traversals is that both querying and transformations can happen in a single pass. The example in listing~\ref{renameVars} renames every \texttt{Name} in an expression by add \texttt{"\_old"} as a suffix and stores the original names in a list. 

\begin{lstlisting}[caption={Changing every found name and storing the old names in a list.},label=renameVars]
renameVars :: Expr -> TransformState Expr
renameVars e = everywhereM (mkM f) e
  where f :: Expr -> TransformState Expr
          f (Assign n e) = do
            modify (\lst -> n:lst)
            return (Assign (n++"_old") e)
          f (Var n) = return (Var (n++"_old"))
          f e = return e
\end{lstlisting}
 
This type of generic traversal is very common in HaRe because traversals often need to make use of a refactoring's stored state or run something from the GHC API which needs the features provided by a \texttt{GhcMonad} all while modifying the abstract syntax.   
	 
\section{ghc-exactprint}

After using generic programming to transform the parsed abstract syntax, HaRe needs to be able to print the modified code. A challenging part of building a refactoring tool is that a user wont want non refactored parts of their code to change at all. HaRe needs to preserve the user's comments and spacing from the source file.

Prior to GHC version 7.10.1 the location of certain keywords and punctuation (such as \texttt{do} and \texttt{let}) and user comments were lost after parsing. This made parsing and then printing an exact copy of a GHC Haskell file impossible. GHC's 7.10.1 release added annotations for the "lost" syntax elements that had previously not been represented in the parsed abstract syntax~\citep{apiAnns}. 

The parser produces a map that associates the keyword and the source span that the keyword can be found in with that keywords exact location. This approach was taken to avoid littering the existing AST with functionally meaningless keyword data~\citep{apiAnns}. 

Even with the position of every syntax element being recorded, printing a module after the AST has been modified is not easy. Many AST elements are "located" with a source span that indicates that element's exact position in the file. This means that any change to the AST will require updating the location of all the syntax elements that occur later in the line at least for single line changes and, in the case of changes that modify entire lines, every element after that change will need its location to be updated.

Ghc-exactprint simplifies this immensely by allowing us to position elements relative to their neighbours rather than absolutely~\citep{exactprint}. After parsing a source file HaRe takes the annotations GHC returns with the parsed abstract syntax and relativise them using ghc-exactprint. For each syntax element ghc-exactprint creates a new data type called an "\texttt{Annotation}" which contains an offset that indicates where this element should be positioned compared to the previous element. Take for example the following definition.

\begin{lstlisting} 
f a = (a+ 1 )
\end{lstlisting}

The absolute position of the plus sign is row one column nine (GHC's locations are one-based) but using ghc-exactprint we instead can think of the position of the plus sign using the offset \texttt{(0,0)} because there is no space between it and the previous element (the variable \texttt{a}). Using this system the number literal following the plus sign has an offset of (0,1) because of the single column of space before it. 

The \texttt{Annotation}s are stored in a map that is keyed based on the parsed location of a syntax element and the string representation of the AST constructor. In the previous example the right hand side of the definition is located at the source span (1,7)-(1,13) which stands for row one columns seven through thirteen and GHC represents this expression with the \texttt{HsPar} which is a constructor for the \texttt{HsExpr} type. The source span and the constructor together can be combined to retrieve the annotation data associated with this bit of the AST. This syntax tree has two elements associated with it, the opening and closing parenthesis. Each of these keywords is given its own offset, which in this case is (0,0) for the opening parenthesis\footnote{The offset in this case is (0,0) and not (0,1) as you might expect because the space between the equals sign and the opening parenthesis is represented in the offset for the entire right hand side expression.} and (0,1) for the closing parenthesis. We could obviously infer from the use of the \texttt{HsPar} constructor that this tree is wrapped in parenthesis however the users specific spacing would be lost without the annotations.

Comments are another element of a source file that prior to GHC 7.10.1 were "lost" after parsing. Using ghc-exactprint comments are stored in the annotations associated with the next piece of syntax. In the following code snippet the comment on line three is added to the annotations associated with the declaration of the function \texttt{f} along with a delta position that indicates the comment is a single line before this declaration starts.

\begin{lstlisting}
type Name = String

--a comment
f i = i + 1
\end{lstlisting}



\section{The current implementation of HaRe}

We have just discussed the major components that HaRe depends on. The ghc-api give us access to the internal representation of Haskell, the generic programming libraries Scrap Your Boilerplate and Strafunski-StrategyLib allows HaRe to more easily work with that internal representation, and ghc-exactprint is how HaRe preserves the source files spacing when writing the output file. From this foundation we can make HaRe focused on refactoring rather than solving these more generic problems. This section will discuss how HaRe is implemented, what its API provides, and some general conventions that it's refactorings use.

\subsection{HaRe's inner workings}\label{hareInners}

The GHC API work within the \texttt{GhcMonad} that provides the features GHC needs to compile a single Haskell source file such as IO, logging warning, exception handling, and keeping track of the compilation session~\citep{ghcApi}. In reality Haskell programs consist of more than just single source files without dependencies. Projects are organised using build tools such as Cabal or Stack~\citep{cabal,stack} that handle these issues for the programmer. HaRe needs to be aware of the context that these tools provide because refactorings may change multiple modules or modify modules that import modules from external dependencies. In HaRe's case ghc-mod provides a monadic context that handles these build environments and compiler setup~\citep{ghcMod}. Within ghc-mod's context HaRe keeps track of the state of the refactoring session.

\begin{lstlisting}[caption={HaRe's Monad \texttt{RefactGhc}},captionpos=b, label=refactghc] 
newtype RefactGhc a = RefactGhc
    { unRefactGhc :: GM.GhcModT (StateT RefactState IO) a}
\end{lstlisting}

Listing~\ref{refactghc} shows the definition of HaRe's monad that each refactoring runs in. \texttt{RefactState} is an ADT that keeps track of all the settings, abstract syntax, and filepath for refactoring a single file. It is possible to refactor client modules by setting the filepath in the \texttt{RefactState} to target another module. HaRe will then parse and typecheck that module's information into the state for transformation.

Even though there are three types of the AST, the annotations are part of the parsed AST ghc-exactprint works with the parsed AST only. Once a refactoring has finished it is expected that the parsed source and annotations will reflect all the changes that the refactoring has made. HaRe's state still contains all three of the syntax trees because the renamed and typed source are useful for the additional information they contain about the source file. 

\subsection{HaRe's API}

Modifying and reasoning about GHC's abstract syntax and maintaining the associated annotations is still a complex task even with help from HaRe's dependencies. HaRe defines its own API to help fill this gap between its dependencies and its refactorings. In addition to the obvious functions that are required for running a refactoring within the \texttt{RefactGhc} monad the API also includes helper functions that make working with the state easier. 

HaRe also defines a large collection of program analysis and transformation functions. For example, pulling the binding of a top level variable from a module's entire abstract syntax tree is a task that many refactorings have to do so this functionality is part of HaRe's API. There are also small program transformations that are not in and of themselves refactorings but common low level modifications that are useful to several refactorings, such as adding a new import declaration or making a function infix by wrapping it in back quotes (the \texttt{`} character).

Additionally there are several transformations that don't affect the abstract syntax as much as they change the annotations that format ghc-exactprint's output. Adding new lines before a syntax element doesn't change the meaning of a program but is important for a refactoring's output to be well formatted and easy to read. 

HaRe's dependencies help abstract away the low-level\footnote{Low level from the perspective of a refactoring at least.} details of a language back-end, build tools, and pretty printing. HaRe's API tries to close the gap still left between the dependencies and the refactorings themselves. In the next section we will take a look at how HaRe's refactorings are implemented.

\subsection{Implementing Refactorings in HaRe}

HaRe actually requires very few things from a refactoring implementation. As was mentioned previously in section~\ref{hareInners} everything must run inside of the \texttt{RefactGhc} monad who's state is where the abstract syntax of a target module is stored. A refactoring is also expected to return a list of  "\texttt{ApplyRefacResult}s" which contains an updated parsed AST and annotations along with the filepath the AST originated from; this updated AST is what HaRe writes out as the result of the refactoring\footnote{HaRe actually outputs to a temporary file. When refactoring "file.hs" HaRe produces "file.refactored.hs." This allows programmers to check the result of a refactoring before overwriting the existing module.}  


Beyond those two features refactorings are free to be implemented however the programmer chooses. However, certain conventions have been adopted within many of HaRe's refactorings. When describing a refactoring one would imagine that checking \textit{pre}conditions would be the first thing the implementation of that refactoring computes. A more efficient implementation checks preconditions throughout computation alongside the AST transformation. Merging precondition checking with transformation saves the refactoring from traversing parts of the AST multiple times, for example, the renaming refactoring checks for name conflicts while it descends the AST replacing the old name with the new one. Obviously this strategy only works for certain preconditions the only precondition for deleting a definition is that the target definition isn't used. The transformation only affects the syntax tree of the definition to be deleted so the implementation of the refactoring has to do a separate scan of the rest of the target module and any of its client modules to determine if the target definition is used or not. 

\chapter{Data-driven refactorings}

\comment{Is this too introductory? Should I move this to the first chapter?}

On the first day of the first programming course of my Bachelors degree my professor tasked us with defining what a "program" is. In the end the classes conclusion was that a program is a process that computes a result from some input. The input and output are the "data" that a program operates over. Through my education I've found that understanding the relationship between input and output is key to writing a program.

The data a program works over highly influences its structure and definition. However one major challenge that programmers face is that there are still multiple correct solutions for many problems though many of those solutions may be suboptimal. On top of this challenge a programmers understanding of the problem could change or the requirements could evolve as the project develops. All of this means that the first implementation of a program is rarely optimal and if it is, it rarely remains optimal as time goes on. 

Data-driven or data-oriented refactorings for functional languages have been discussed in the literature for some time. Notable examples include work on monadification(\cite{monadification},\cite{monadSurvey}) and transforming datatypes~(\cite{datatypeTransformation}, \cite{brownThesis}).  The goal of this thesis is to refine this class of refactorings.

This chapter will introduce what I mean by "data-driven" refactorings. The chapter begins with discussion of how these object-oriented refactorings translate (or don't) to a functional language. From there the chapter will give several examples of data-driven refactorings for Haskell.

\section{Object-Oriented Data Refactorings}\label{ooRefs}

The origins of refactoring are deeply rooted in the object-oriented world~(\cite{programRestructuring},~\cite{refactOOFrameworks}). The canonical catalogue of refactorings remains Martin Fowler's \underline{Refactoring: Improving the Design of Existing Code} ~\citep{fowler}. Fowler's catalogue of refactorings are all written in Java though he purposefully avoided using any features that were unique to Java so that the refactorings could be useful in many different programming languages.

As a functional programmer when going through this catalogue of refactorings there seem to be three types of refactorings.

\begin{itemize}
	\item Refactorings that are applicable in a functional language
	\item Refactorings not applicable to a functional language
	\item Refactorings that could be adapted for use in a functional language
\end{itemize}

That first type of refactorings' usefulness to a functional program is easily understood. Refactorings like renaming or adding a parameter don't depend on object-oriented features of the target language. 

The second type of refactorings are so dependent on features associated with object-oriented languages that they are impossible to implement or are meaningless in a functional language. The "remove setting method" refactoring depends on the common OO pattern of each field of a class having "getter" and "setter" methods that retrieve or modify that field respectively. This pattern, on the other hand, is not as ubiquitous in functional languages because many do not support objects and immutability makes a "setter" function not in the functional style. OCaml would be a notable exception to this rule. OCaml supports objects and allows the programmer to mark variables as mutable so getter and setter methods are possible.

\begin{lstlisting}[caption={An OCaml object with getter and setter methods.},captionpos=b, language=caml, morekeywords={object,method},label=ocamlObj]
let mInt init_i = object
    val mutable i = init_i

    method get_i = i
    method set_i new_i =
      i <- new_i
  end
\end{lstlisting}

This OCaml object from Listing~\ref{ocamlObj} would be a valid target for this refactoring. However these methods are not as ubiquitous as they are in imperative object-oriented languages with mutable data as the default so a refactoring to remove a setter method is of limited value even for OCaml.

The third type of refactoring found in~\citep{fowler} is much more interesting to a functional programmer. The specifics of these refactorings aren't directly applicable to functional programs but the underlying motivations are relevant to any programming paradigm. 

For example here's the motivation for the "Replace Data Value with Object" refactoring in~\citep[pg. 175]{fowler}:

\begin{displayquote}
Often in early stages of development you make decisions about representing simple facts as simple data items. As development proceeds you realize that those simple items aren't so simple anymore. A telephone number may be represented as a string for a while, but later you realize that the telephone needs special behavior for formatting, extracting the area code, and the like. For one or two items you may put methods in the owning object, but quickly the code smells of duplication and feature envy. When the smell begins, turn the data value into an object.
\end{displayquote}

This refactoring extracts a field that was some primitive type into an object. The example from~\citep{fowler} works over an order class with a string that representing the customer that placed the order.

\begin{lstlisting}[caption={The Order class}, language = java, captionpos=b,tabsize=4]
class Order {
	public Order (String customer) {
		_customer = customer;	
	}
	
	public String getCustomer() {
		return _customer;
	}
	
	public void setCustomer(String arg){
		_customer = arg;	
	}
	
	private String _customer;
}
\end{lstlisting}

The refactoring creates a new customer class that just has a string field with a getter method as seen in Listing~\ref{custCls}. The customer class doesn't add any additional features but the extra layer of abstraction sets up the code base for further development. The customer object could have fields added that represent contact info or further demographic information without polluting the order class with non order relevant data.

\begin{lstlisting}[caption={The result of the Replace Data Value with Object refactoring when applied to the customer field of the order class.}, language = java, captionpos=b,label=custCls,tabsize=4]
class Order {
	public Order (String customer) {
		_customer = new Customer(customer);	
	}
	
	public String getCustomer() {
		return _customer.getName();
	}
	
	public void setCustomer(String arg){
		_customer = new Customer(arg);	
	}
	
	private Customer _customer;
}

class Customer {
	public Customer(String name){
		_name = name;
	}
	
	public String getName() {
		return _name;
	}
	
	private final String _name;
}
\end{lstlisting}

Functional programmers have to make similar data representation decisions as the object-oriented programmer. At the start of a project representing a customer just by their name could be reasonable. As the project develops this can become a serious limitation and a more robust abstraction is required.

This section introduced how object-oriented refactorings help build up the data model over a projects lifetime. In an object-oriented language the primary abstraction method is to just introduce additional objects. These sorts of refactorings for a language with a rich type system like  Haskell offers many more choices for evolving a system's data model. The rest of this chapter will describe refactorings for Haskell that support this evolution.  

\section{Data-Driven Refactorings in Haskell}

Haskell offers a rich environment for data representation. The Haskell 2010 standard defines several types that will be included in the prelude including tuples, lists, characters, strings (which are just lists of characters), several types of numbers, and the function type. Additionally programmers can construct new types with algebraic data types or rename an existing type with type synonyms. Type classes supports overloading as well. The standard library of GHC comes with many type classes that can help produce powerful abstractions~\citep{typeclassopedia}. Contrast this with how most object-oriented type systems are either unified where every type is a subclass of some top level \texttt{Object} class (e.g. C\# or Ruby) or there are a set of predefined primitive types which cannot be changed as well as the object hierarchy (e.g. C++ and Java's type systems). 

These two different approaches to type systems both have pros and cons which has sparked a vigorous (and quite possibly eternal) debate. The goal of this thesis is not to add to this debate, instead hopefully a few things are clear at this point.

\begin{itemize}
	\item Data representation is a language independent problem that must be answered in every project.
	\item The way a project manages and represents its data will (and should) evolve over a project's lifetime.
	\item Refactoring is a structured way to support this evolution.
	\item Refactorings for a language like Haskell need to take a different approach than those for object-oriented languages.
\end{itemize} 

\subsection{Introducing a Type Synonym}\label{introSyn}

Type synonyms in Haskell, as mentioned previously, are a way to name an existing type. A simple example can be seen in listing~\ref{fooSyn}. 

\begin{lstlisting}[label=fooSyn,caption={A simple type synonym.}]
type Foo = (String, Int)

f :: Foo -> Foo
f x@(_, 0) = x
f (str, i) = (tail str, i-1) 
\end{lstlisting}

Any place that where the \texttt{Foo} synonym is in scope the new name can be used to refer to any value of type \texttt{(String, Int)}. In fact "\texttt{Foo}s" and "\texttt{(String, Int)}s" are completely interchangeable. Introducing a synonym is a good way to quickly and simply name types to suggest their specific use in the current application.

Returning to the example from~\citep{fowler} used in section~\ref{ooRefs} of an order type that keeps track of the customer who placed the order (among other things presumably but they have been left out here for brevity's sake), see listing~\ref{haskellOrder} for a Haskell implementation of this type and a function that counts how many orders a particular customer has placed in a list of orders.

\begin{lstlisting}[label=haskellOrder,caption={An order algebraic data type.}]
data Order = Order {customer :: String}

numberOfOrdersFor :: [Order] -> String -> Int
numberOfOrdersFor orders name = length (filter (\ord -> name == (customer ord)) orders)
\end{lstlisting}

The current representation of a customer as just a \texttt{String} is a bit underdeveloped as it is. Introducing a customer synonym is a simple step that sets up the code base for further development. The synonym will help clearly mark which strings in the program stand for customers and which are  not.

The introduce a type synonym refactoring works by taking a type and a valid synonym name (as per the Haskel 2010 standard~\citep{haskell2010}) and creates a new synonym. In this case the type is \texttt{String} and the synonym name should be something like "\texttt{Customer}." The only precondition of the refactoring is that the new synonym name cannot cause a name clash. 

\begin{lstlisting}[caption={The customer synonym}]
type Customer = String
\end{lstlisting}

After this the next part of the refactoring involves replacing the appropriate uses of the type with the new synonym. This part of the refactoring is difficult to automate and needs to be interactive\footnote{This is not a feature of HaRe yet.}. There is no way to infer which instances of \texttt{String} (in the case of this example) should be replaced with the \texttt{Customer} synonym. The code from listing~\ref{haskellOrder} can have all of the string instances replaced by \texttt{Customer} because all the strings are being used to represent one. If there were a second function \texttt{printThankYou} which has type \texttt{String -> Order -> IO ()} which prints out a customized thank you message to the customer from the business for their order. 

\begin{lstlisting}[caption={The printThankYou function}]
printThankYou :: String -> Order -> IO ()
printThankYou businessName order = do
	putStrLn ("Thank you " ++ (customer order) ++ " for your order.")
	putStrLn (businessName ++ " hopes to see you again soon!")
\end{lstlisting}

Though the first argument to \texttt{printThankYou} is a \texttt{String} it does not represent a customer therefore shouldn't be replaced by the new synonym. The implicit meaning of the first argument hasn't been encoded in the type system so the programmer has to intervene during the refactoring to make their intention for each instance of \texttt{String} clear. The final result of the refactoring can be seen in listing~\ref{orderRefact}. 

\begin{lstlisting}[label=orderRefact,caption={The final result after adding a synonym for String.}]
type Customer = String

data Order = Order {customer :: Customer}

numberOfOrdersFor :: [Order] -> Customer -> Int
numberOfOrdersFor orders name = length (filter (\ord -> name == (customer ord)) orders)

printThankYou :: String -> Order -> IO ()
printThankYou businessName order = do
	putStrLn ("Thank you " ++ (customer order) ++ " for your order.")
	putStrLn (businessName ++ " hopes to see you again soon!")
\end{lstlisting}

 This transformation might seem like too small of a step. Wouldn't it be preferable to introduce a more powerful abstraction such as an algebraic data type? One of the principles of the Agile Manifesto is "simplicity" which is described as "maximizing the amount of work not done is essential"~\cite{agileManifesto}. The work not done in this case is the introduction of a more complex customer representation. This small step does clearly differentiate the strings that represent customers from other strings that represent other types of data.


\section{Generalising Maybe}
 
A common data refactoring is generalisation. Generalisation is taking code written for a specific type and rewriting it to use a more general type. The newly generic code is applicable in more places and can help reduce code duplication.\underline{Refactoring} dedicates an entire chapter to generalisation cataloguing refactorings like "extract subclass" and "extract interface"~\citep[pg. 319]{fowler}. 

Generalisations for object-oriented languages either move functionality up in a hierarchy (as in "push method up") or change the hierarchy by adding classes (e.g. "extract subclass"). Object-oriented languages have a single hierarchy since every object inherits from a root class typically just called \texttt{Object}. Functional languages don't have this single unified hierarchy, but smaller hierarchies do exist because type classes can inherit from one another. 

This thesis will outline two generalisations, one makes code of a specific type work over a type class instead (this section), the other generalises code of one type class to use a class higher the hierarchy(covered in chapter~\ref{applicative}). This section describes a refactoring that rewrites programs of type \texttt{Maybe a} to use the \texttt{MonadPlus} or, if possible, the \texttt{Monad} type classes. 

\begin{lstlisting}[caption={The \texttt{Maybe} data type definition and \texttt{MonadPlus} instance declaration, and the \texttt{MonadPlus} class definition.},label=maybeMonadPlus]
data Maybe a = Nothing
                 | Just a
                 
instance MonadPlus Maybe where
   mzero = Nothing
   Nothing `mplus` r = r
   l          `mplus` _ = l
                 
class Monad m => MonadPlus m where
   mzero :: m a
   mplus :: m a -> m a -> m a
\end{lstlisting}
 
Listing~\ref{maybeMonadPlus} contains the \texttt{Maybe} instance of and the class declaration for \texttt{MonadPlus}. The \texttt{Maybe} type represents a computation (returned wrapped in the \texttt{Just} constructor) that can fail (represented by the \texttt{Nothing} constructor). The \texttt{MonadPlus} class is a typeclass for monads that also have a monoidal structure~\footnote{A monoid is a semigroup who's associative binary operation has an identity element.}. 

The \texttt{MonadPlus} class helps generalise monads that contain some concept of failure and choice. The \texttt{mzero} value represents a failed computation and \texttt{mplus} represents a way of making a "choice" between two computations that may or may not have failed~\citep{typeclassopedia}. 

\begin{lstlisting}[caption={\texttt{Maybe}'s monad definition}, label=maybeMonad]
class Applicative m => Monad m where
	return :: a -> m a
    (>>=)  :: forall a b. m a -> (a -> m b) -> m b
    
instance Monad Maybe where
	return = Just
	
	(Just a) >>= f = f a
	Nothing  >>= _ = Nothing
\end{lstlisting}

\subsubsection{Generalising Maybe to Monad}\label{genMonad}

This refactoring replaces \texttt{Maybe} specific code with the more general \texttt{Monad} and \texttt{MonadPlus} operations. This is done by recognizing when \texttt{Maybe} specific code is structured like the more general operations. Listing~\ref{mmp1} contains a simple example of this.  

\begin{lstlisting}[caption={\texttt{inc}},label=mmp1]
inc :: Maybe Int -> Maybe Int
inc Nothing = Nothing
inc (Just i) = (Just (i + 1))
\end{lstlisting}

The function \texttt{inc} can be rewritten to use the monadic operations bind (\texttt{>>=}) and \texttt{return} instead, this is because \texttt{inc}'s definition matches \texttt{Maybe}'s definiton of bind. Listing~\ref{incRewrite} has another version \texttt{inc} of so that relationship is clearer. 

\begin{lstlisting}[caption={\texttt{inc} rewritten to look more like bind},label=incRewrite]

inc :: Maybe Int -> (Int -> Maybe Int) -> Maybe Int
Nothing `inc` _ = Nothing
(Just i) `inc` f = f i

f i = Just (i + 1)
\end{lstlisting}

The new version of \texttt{inc} has been written using infix notation, to more closely match \texttt{Maybe}'s definition of bind. Also like \texttt{Maybe}'s bind when \texttt{inc}'s first argument is \texttt{Nothing} it just returns \texttt{Nothing}. Finally the right hand side of \texttt{inc}'s second case was lifted into a separate function and is passed to the new \texttt{inc} as an argument, just like bind.

In practice the refactoring will, by default, create an anonymous function from the right hand side of the \texttt{Just} case with calls to \texttt{Just} and \texttt{Nothing} replaced with \texttt{return} and \texttt{mzero} respectively. The final refactored version of \texttt{inc} is in listing~\ref{mmp1Ref} 

\begin{lstlisting}[caption={Final output from generalising \texttt{inc}},label=mmp1Ref]
inc :: (Monad m) => m Int -> m Int
inc mi = mi >>= (\i -> (return (i+1)))
\end{lstlisting}

It is worth saying that if the right hand side of the \texttt{Just} case from the original implementation of \texttt{inc} contained calls to \texttt{Nothing} then this function could not be generalised to \texttt{Monad}. Instead, the anonymous function would replace the occurrences of \texttt{Nothing} with \texttt{mzero} instead.  
 
\subsubsection{Generalising Maybe to MonadPlus} 

The first example could be rewritten using functionality provided only by the \texttt{Monad} type class. Obviously this isn't always the case. An example of a function that cannot be completely generalised to \texttt{Monad} is in listing~\ref{mmp2}.

\begin{lstlisting}[caption={\texttt{showNat}}, label=mmp2]
showNat :: Int -> Maybe String
showNat i =
  if (i <= 0)
    then (Just (show i))
    else Nothing
\end{lstlisting}

The function \texttt{showNat} takes in a pure value and returns a \texttt{Maybe}, instead of taking in a \texttt{Maybe} and returning another \texttt{Maybe} as in the previous example. The more general version of \texttt{showNat} needs to be able to express the idea of failure that \texttt{MonadPlus} encodes with the \texttt{mzero} operation. 

\begin{lstlisting}[caption={\texttt{showNat} refactored}, label=mmp2Ref]
showNat :: (MonadPlus m) => Int -> m String
showNat i =
  if (i <= 0)
    then (return (show i))
    else mzero
\end{lstlisting}
 
Listing~\ref{mmp2Ref} shows the refactored version of \texttt{showNat}. In this case the refactoring can simply replace the \texttt{Maybe} specific calls with more general ones. This is very similar to the way that the expression that made up the anonymous function from listing~\ref{mmp1Ref} was constructed by changing calls to \texttt{Just} to \texttt{return} and calls to \texttt{Nothing} to \texttt{mzero}. 

Due to \texttt{Monad} and \texttt{MonadPlus} both of \texttt{Maybe}'s constructors can be replaced with a more general operation. However if a function deconstructs a \texttt{Maybe} type through pattern matching it may not be generalisable. Consider the function \texttt{printResult} in listing~\ref{printRes}.

\begin{lstlisting}[caption={\texttt{printResult}},label=printRes]
printResult :: (Show a) => Maybe a -> IO ()
printResult m =
  case m of
     Nothing -> putStrLn "Something went wrong"
     (Just i) -> putStrLn $ "The result is: " ++ (show i)
\end{lstlisting}

\texttt{printResult} uses \texttt{Maybe}'s constructors for pattern matching rather than in expressions; there is no generic pattern match for \texttt{Just}. Refactoring \texttt{printResult} will require the programmer to provide a way to convert its argument to \texttt{Maybe}.

\begin{lstlisting}[caption={Refactored \texttt{printResult}},label=printRef]
printResult :: (Show a, MonadPlus m) => (m a -> Maybe a) -> m a -> IO ()
printResult f = printResult_old . f
   where
	printResult_old :: Maybe a -> IO ()
   	printResult_old m =
   	  case m of
  	  Nothing -> putStrLn "Something went wrong"
  	  (Just i) -> putStrLn $ "The result is: " ++ (show i)
\end{lstlisting}
 
Listing~\ref{printRef} show the finished refactoring of \texttt{printResult}. The original definition of \texttt{printResult} is renamed and moved into a local definition. The new \texttt{printResult} function now takes in an additional parameter, an "abstraction function." The abstraction function abstracts the new type \texttt{m a} back to the old \texttt{Maybe a}. The inverse of the abstraction function is the projection function\footnote{In this specific case the projection function is of type: \texttt{Maybe a -> m a}}. These types of functions are used more in the next section (\ref{listToDlist}).

When passed the abstraction function \texttt{printResult} just converts the now genericly typed argument to \texttt{Maybe a} and then calls the original function definition.
 
\section{Lists to Hughes Lists a Refactoring}\label{listToDlist}

The previous section discussed a refactoring that is most useful in the early stages of development when the details of data representation are in their infancy. As the project develops more and more decisions must be made about how data is stored and processed. Mid-development it may become necessary to change what data structure the program is used.

This section will cover a refactoring for automatically replacing a type by "projecting" that type into another. The two types (the original and the new type) don't necessarily to have a formal relationship from the compiler's point of view (e.g. both implement the same type class) but instead have a similar "interface" for to interact with. 

\subsection{Hughes Lists}

Appending two lists into a single list is a basic and key operation. The standard implementation of append is seen in listing~\ref{concat}. 

\begin{lstlisting}[caption={The standard definition of concat},label=concat]
(++) :: [a] -> [a] -> [a]
[]  ++ ys = ys
(x:xs) ++ ys = x:(xs ++ ys)
\end{lstlisting}

If the first argument to append is the empty list then append can just return the second argument. In the other case append traverses the first list popping off the head of the first list and recursively appending the tail of the first list and the second argument. The performance of append is proportional to the length of its first argument. This has the unfortunate side effect where if a program builds of a list by repeatedly appending  to the end of a list the program will spend significant amounts of time traversing the beginning of the list over and over. In total the performance of this function ends up being $O(n^2)$ where $n$ is the length of the final list.

Fortunately there is an alternative representative of lists that allows $O(n)$ time appends. This alternative representation was first described by John Hughes in~\citep{hughesList} (hence their name), they are also known as difference lists~\citep{realWorldHaskell}; difference lists is the name that they are provided by in Hackage~\citep{dlist}. In Hughes lists elements are stored as partial applications of the append function, these partial applications are then composed together using function composition (the \texttt{(.)} operator in Haskell). 

Difference lists store the values \texttt{[1,2,3]} as \texttt{([1,2,3] ++)} which is of type \texttt{Num a => [a] -> [a]}. Appending \texttt{[4,5,6]} to \texttt{([1,2,3] ++)} first involves converting it to a difference lists (\texttt{([4,5,6] ++)} in this case) then these two difference lists can be appended with function composition which results in \texttt{([1,2,3] ++) . ([4,5,6] ++)}. 

\begin{lstlisting}[caption={Building and deconstructing difference lists.},label=ghciDList]
> let lst = ([1,2,3] ++) . ([4,5,6] ++)
> :t lst
lst :: Num a => [a] -> [a]
> lst []
[1,2,3,4,5,6]
\end{lstlisting}

As seen in listing~\ref{ghciDList}, once it comes time to retrieve the normal list from a difference list applying an empty list to the difference list and the partial applications are evaluated from right to left. Internally difference lists are just a wrapper around a function from lists to lists. Listing~\ref{dlistDef} shows the definition of the \texttt{DList} new type which contains the partial application. The \texttt{unDL} function simply removes the \texttt{DL} constructor. 

\begin{lstlisting}[caption={The definition of \texttt{DList} taken from~\citep{realWorldHaskell}}, label=dlistDef]

newtype DList a = DL {
   unDL :: [a] -> [a]
}

fromList :: [a] -> DList a
fromList xs = DL (xs ++)

toList :: DList a -> [a]
toList (DL xs) = xs []

append :: DList a -> DList a -> DList a
append xs ys = DL (unDL xs . unDL ys)
\end{lstlisting}

While difference lists support fast appends there is no such thing as a free lunch, speed ups for certain functions are paid for by slowdowns in other places. Getting the head and tail of a normal list are both constant time operations but become linear time for difference lists because the difference lists will have to be converted back to normal lists. 


\subsection{Refactoring lists to Hughes lists}

For functional programmers, lists are a very familiar and versatile data structure. However, if an application requires repeated appends as described at the start of this section their performance becomes an issue. Difference lists provide a similar interface to lists but without this troublesome behaviour. This section will describe a refactoring to convert programs written using normal lists to instead use difference lists. 

\subsubsection{Isomorphic Types} 
This refactoring takes the view that a type consists of some structure and a set of functions that operate on that structure. 

Two types, \textit{A} and \textit{B} are considered isomorphic if there exists a projection function \textit{proj :: A -> B}, an abstraction function \textit{abs :: B -> A}, and a set of pairs where each pair consists of one function that operates over \textit{A} and the function that performs the same operation over \textit{B}. For example, if type \textit{A} had a function \textit{add} and \textit{B} has a function \textit{insert} that both add a new element to the respective collections these two functions should be paired. The refactoring modifies the target function(s) by replacing functions using the old type with the function they are paired with. If functions exist that are not in the set of pairs then the projection and/or abstraction functions are inserted to change the type of that particular branch of the abstract syntax tree.

In the list to Hughes list case the projection function is \texttt{fromList} (because it projects lists into the new type \texttt{DList}) and the abstraction function is \texttt{toList}. What to define the set of pairs as is an interesting problem with multiple "correct" solutions. The \texttt{Data.DList} module exports the following functions~\citep{dlist}.

\begin{center}
\begin{tabular}{| c | c | c |}
  \hline
  \texttt{apply} & \texttt{empty} & \texttt{singleton}\\
  \hline
  \texttt{cons} & \textbf{\texttt{snoc}} & \texttt{append} \\
  \hline
  \texttt{concat} & \texttt{replicate} & \textbf{\texttt{list}}\\ 
  \hline	
  \texttt{head} & \texttt{tail} & \texttt{unfoldr}\\ 
  \hline  
  \texttt{foldr} & \texttt{map} \\
  \hline
\end{tabular}
\end{center}

From this list the two bolded functions\footnote{(\texttt{snoc :: DList a -> a -> DList a}) appends a single element to a list}\footnote{(\texttt{list :: b -> (a -> DList a -> b) -> Dlist a -> b}) is list elimination for dlists} are the only functions without normal list counterparts. The rest of the \texttt{DList} API could be paired with equivalent list functions. However, its not necessarily a good idea for every normal list function to be refactoring to its \texttt{DList} equivalent. As was mentioned certain \texttt{DList} functions are less efficient than the corresponding normal function, the primary purpose of this refactoring is defeated if the refactored code runs slower than the original source.

Fortunately different versions of the refactoring can be made by defining separate sets depending on the behaviour that is desired. For example one set could only include the \texttt{DList} constant time operations (\texttt{append}, \texttt{empty}, and \texttt{cons}) and another set could include all possible pairings. 

\subsubsection{Transforming functions}

The refactoring breaks into three different cases depending on the type of the target function. 

In section~\ref{introSyn} there was a point when the refactoring required the user to tell HaRe where the new synonym should be used because the tool does not know what is the user's intention for the synonym. Many data-driven refactorings need to either make assumptions about what the user's intent is or directly solicit information from them. This refactoring is no different, for any given function there multiple correct definitions. Take for example the function \texttt{insComma} in listing~\ref{insComma}.

\begin{lstlisting}[label=insComma,caption={\texttt{insComma}}]
insComma :: String -> String -> String
insComma s1 s2 = s1 ++ "," ++ s2
\end{lstlisting}  

When refactoring both of the arguments to and the result type of \texttt{insComma} to all become \texttt{DList Char} there are multiple ways to refactor this function. Listing~\ref{commaRef} shows two possibilities, which one should the refactoring produce and why?

To make this decision, the refactoring must infer that because this function is being refactored to use the new type the user wants the new type to be used in as many places as possible. This makes the first definition preferable to the second one because it only converts a single item using \texttt{fromList} and replaces the appends whereas the second example converts the two arguments into lists appends everything together and then converts the result back into a \texttt{DList}. The refactoring prioritises minimising the amount of conversions introduced into the refactored program.

\begin{lstlisting}[label=commaRef,caption={Two possible refactorings for \texttt{insComma}}]
insComma_1 :: DList Char -> DList Char -> DList Char
insComma_1 s1 s2 = s1 `append` fromList (",") `append` s2

insComma_2 :: DList Char -> DList Char -> DList Char
insComma_2 s1 s2 = fromList ((toList s1) ++ (",") ++ (toList s2))
\end{lstlisting}

\subsubsection{Example 1}

The simplest case of this refactoring is modifying the type of a parameter. Consider the example in listing~\ref{median} that calculates the median of list of numbers. 

\begin{lstlisting}[caption={Calculating a median}, label=median]

median :: Fractional a => [a] -> a
median lst = foldr (+) 0 lst / length lst

\end{lstlisting} 

Refactoring \texttt{median}'s first argument to become a Hughes list is fairly straight forward matter of wrapping the, now of type \texttt{DList}, parameter \texttt{lst} with the abstraction function \texttt{toList}. The refactored version of \texttt{median} is in listing~\ref{medianRef}. 

\begin{lstlisting}[caption={\texttt{median} refactored}, label=medianRef]

median :: Fractional a => DList a -> a
median lst = foldr (+) 0 (toList lst) / length (toList lst)

\end{lstlisting}    

This example is one of the cases where there are multiple possible refactorings. Listing~\ref{medianRef2} shows a different version of a refactored median, because \texttt{foldr} is defined both for difference lists and normal lists the abstraction function could be added around that expression instead. Which version should the refactoring produce?

\begin{lstlisting}[caption={\texttt{median} refactored another way.}, label=medianRef2]

median :: Fractional a => DList a -> a
median lst = toList (foldr (+) 0 lst) / length (toList lst)

\end{lstlisting}    

In this case where the refactoring targets a parameter and the function's result is a type other than a Hughes list the refactoring assumes that the programmer wants the Hughes list converted as soon as possible in the function. 

\subsubsection{Example 2}

The second example covers the case where the result type of a function is refactored to use \texttt{DList} instead of list. Listing~\ref{enumBefore} contains the definition of a simple algebraic data type of a tree, a function (\texttt{enumerate}) that returns an in-order list of all the tree's elements, and a function that prints a tree's enumeration to standard output. 

This time the refactoring will affect the result type of the function. As opposed to the previous example where changes occurred at the leaves of the abstract syntax tree this case of the refactoring changes the type of the AST's root. 

\begin{lstlisting}[caption={Definition of enumerate}, label=enumBefore]
data Tree a = Leaf
            | Node (Tree a) a (Tree a)

enumerate :: Tree a -> [a]
enumerate Leaf = []
enumerate (Node left x right) = (enumerate left) ++ [x] ++ (enumerate right)

printEnumTree :: (Show a) => Tree a -> IO ()
printEnumTree tree = let lst = enumerate tree in
  print lst
\end{lstlisting}

Changing the type of the AST's root requires the refactoring to traverse the tree top down from left to right. This is because the result type of any syntax tree is determined by the function (or value in the case of tree with only a single node) in the leftmost child. 

\texttt{enumerate}'s first case is simple enough to refactor. There is only a single value in the tree the empty list literal. This node's current type is \texttt{[a]} and it needs to become \texttt{DList a}. When the refactoring reaches the \texttt{[]} value it searches to see if it is paired with some difference list operation in the set of pairs. The empty list literal is paired with the difference list operation \texttt{empty}, the refactoring sees this and replaces the empty list with \texttt{empty}.

After the replacement of \texttt{[]} \texttt{enumerate}'s first case is successfully refactored. The second case of \texttt{enumerate} is more complex than the first. Figure~\ref{enumAST} shows a syntax tree for \texttt{enumerate}'s second case. 

\begin{figure}[h]\label{enumAST}
	\begin{center}
		\includegraphics[scale=.5]{graphVis/Chapter3/enumerate.png}
	\end{center}
	\caption{A simplified syntax tree of \texttt{enumerate}'s second case.}
\end{figure}

The refactoring begins the traversal at the top of the syntax tree with the goal of modifying the entire tree to have a result type of \texttt{DList a}. The root node of the tree is the operator application of the left append operation. This append's result type is the type that determines the whole tree's result type. The standard append operation is paired with the difference list \texttt{append} operation. The refactoring checks to ensure that the difference list append has the correct result type and because it does makes the replacement.\footnote{The refactoring will automatically make \texttt{append} infix by surrounding the call with backtick characters since this replacement is modifying an operator application.} After replacing \texttt{++} with \texttt{append} the result type is correct but the function will no longer type check because \texttt{++} and \texttt{append}'s arguments are not the same types. 

The refactoring then must recurse down both the left and right subtrees to change their types from \texttt{[a]} (the type of \texttt{(++)}'s arguments) to \texttt{DList a} (the type \texttt{append}'s arguments). 

\begin{figure}[h]\label{enumLeft}
	\begin{center}
		\includegraphics[scale=.5]{graphVis/Chapter3/enumLeft.png}
	\end{center}
	\caption{The left subtree of \texttt{enumerate}'s second case.}
\end{figure}

The left subtree of the root node is shown in figure~\ref{enumLeft}. The refactoring can descend through the parenthesis (represented by the \texttt{HsPar} constructor); the refactoring continues down the left side of the function application (\texttt{HsApp}). When the refactoring encounters the call to \texttt{enumerate} it recognises that this is the recursive call and even though the type of \texttt{enumerate} stored in the syntax tree is \texttt{Tree a -> [a]} after the refactoring its type will be \texttt{Tree a -> DList a} which is the correct result type  for this subtree. The refactoring can now confirm that the left subtree is of type \texttt{DList a} without checking the right side of the application because the refactoring didn't change the type of \texttt{enumerate}'s arguments.

\begin{figure}[h]\label{enumRight}
	\begin{center}
		\includegraphics[scale=.5]{graphVis/Chapter3/enumRight.png}
	\end{center}
	\caption{The right subtree of \texttt{enumerate}'s second case.}
\end{figure}

After refactoring the left subtree the right subtree needs to be modified to be of type \texttt{DList a} as well. Shown in figure~\ref{enumRight}, the right subtree's root is the second call to \texttt{(++)}. Once again the refactoring replaces \texttt{++} with \texttt{append} because \texttt{append}'s result type is also \texttt{DList a}. Doing this replacement sets off additional traversals that need to ensure that the two arguments to the root node become typed \texttt{DList a} as well. The right argument to the root of this subtree is the \texttt{(enumerate right)} expression which is handled in the same way the \texttt{(enumerate left)} call was handled. The left subtree of figure~\ref{enumRight}  is the list literal \texttt{[x]}. This can be replaced with a call to \texttt{(singleton :: a -> DList a)} the equivalent difference list function. At this point the refactoring is finished modifying \texttt{enumerate} and the result can be seen in listing~\ref{enumRef}.

\begin{lstlisting}[caption={The refactored definition of \texttt{enumerate}}, label=enumRef]
enumerate :: Tree a -> DList a
enumerate Leaf = empty
enumerate (Node left x right) = (enumerate left) `append` (singleton x) `append` (enumerate right)
\end{lstlisting}

The refactoring isn't finished yet, however. The original definition contained in listing~\ref{enumBefore} had another function \texttt{printEnumTree} that depended on \texttt{enumerate}. The final modification that needs to happen to this example is to wrap all calls to \texttt{enumerate} in \textit{non-refactored} function definitions with the abstraction function to convert the result back to a list. The final product of the refactoring can be seen in listing~\ref{enumFinal}.

\begin{lstlisting}[caption={The final product of the refactoring}, label=enumFinal]
data Tree a = Leaf
            | Node (Tree a) a (Tree a)

enumerate :: Tree a -> DList a
enumerate Leaf = empty
enumerate (Node left x right) = (enumerate left) `append` (singleton x) `append` (enumerate right)

printEnumTree :: (Show a) => Tree a -> IO ()
printEnumTree tree = let lst = toList (enumerate tree) in
  print lst
\end{lstlisting}

\subsubsection{Example 3}

The final case of this refactoring involves modifying both the result type and one (or more) of the parameters of the target function. This example will use the \texttt{explode} function from listing~\ref{explode} to refactor both its argument and result type to become \texttt{DList a}.

\begin{lstlisting}[caption={The initial definition of \texttt{explode}},label=explode]
explode :: [a] -> [a]
explode lst = concat (map (\x -> replicate (length lst) x) lst)
\end{lstlisting}

The abstract syntax tree of explode is in figure~\ref{explode}.

\begin{figure}[!h]\label{explode}
	\begin{center}
		\includegraphics[scale=.5]{graphVis/Chapter3/explode.png}
	\end{center}
	\caption{\texttt{explodes}'s syntax tree}
\end{figure}

Much like the previous case where just the result type was modified the refactoring will start working on the syntax tree in a top down manner modifying the function definition to have the correct result type. The refactoring starts on the left subtree with the call to \texttt{concat :: Foldable t => t [a] -> [a]}. The equivalent difference list function is \texttt{concat :: Foldable t => t (DList a) -> DList a} since the result type of this version of \texttt{concat} is \texttt{DList a} the refactoring performs the switch. 

After the change on the left subtree the refactoring needs to modify the right subtree so that is is of type \texttt{Foldable t => t DList a} rather than its current type of \texttt{Foldable t => t [a]}. The left most child of the right subtree is the call to \texttt{map :: (a -> b) -> [a] -> [b]}. The difference list equivalent map is appropriately typed \texttt{map :: (a -> b) -> DList a -> DList b}. Should the refactoring change this node to the difference list version? And if so what changes will be need to made to other subtrees?

The refactoring will swap this node out if \texttt{Foldable t => t (DList a)} (the type of \texttt{concat}'s parameter) can be the same type as \texttt{DList b} (\texttt{map}'s result type); since \texttt{DList} is a member of the \texttt{Foldable} type class the swap can happen as long as the \texttt{b} type variable in \texttt{map}'s type is equal to \texttt{DList a}. This node's type after the swap and filling in the known type variables is: \texttt{map :: (a -> DList b) -> DList a -> DList (DList b)} this new type's arguments' types are both different from the original types so the refactoring will need to check both of these subtrees as well. 

\begin{figure}[h]\label{explodeLam}
	\begin{center}
		\includegraphics[scale=.5]{graphVis/Chapter3/explodeLam.png}
	\end{center}
	\caption{The syntax tree of the lambda expression in \texttt{explode}.}
\end{figure}

Starting with the lambda expression (the syntax tree in figure~\ref{explodeLam}) the refactoring needs to modify it's type to become \texttt{a -> DList b}\footnote{\texttt{a} and \texttt{b} could be the same type.}. The current type of the lambda expression is \texttt{a -> [a]} so the refactoring only needs to modify its result type. The refactoring can then proceed to the left-most child of this expression, the call to \texttt{replicate :: Int -> a -> [a]}. The refactoring swaps this call for the difference list version of \texttt{replicate :: Int -> a -> DList a}. 

If this refactoring were only modifying the result type of \texttt{explode} the refactoring would be done modifying the lambda expression because the type change affects the syntax tree in a top down manner. In this case, however, because  the type of \texttt{explode}'s argument was also changed so leaves of the syntax tree can also have changed type. This means that every subtree needs to be checked to ensure that it still type checks.

The untouched sections of this expression's syntax tree are \texttt{replicate}'s arguments. The second argument is just the variable \texttt{x} which hasn't changed type. The first argument on the other hand is the expression \texttt{(length lst)} and \texttt{lst} is the argument of \texttt{explode} that is now a difference list rather than a normal list. Ideally the refactoring would rewrite this expression by replacing the normal list functions (\texttt{length} in this case) with difference list equivalents. Unfortunately there is no difference list version of \texttt{length} so the refactoring will have to introduce the abstraction function to convert back to a normal list to calculate its length. 

The lambda expression has now been successfully refactored. The rewritten lambda expression is in listing~\ref{lamRef}.\footnote{To help clarify which functions are for lists and which are the difference list versions, all difference list functions have been qualified with the \texttt{DList} identifier.}

\begin{lstlisting}[caption={The refactored lambda expression.},label=lamRef]
(\x -> DList.replicate (length (DList.toList lst)) x)
\end{lstlisting}

Once refactoring the lambda expression is finished there is only a single node of \texttt{explode} that the refactoring hasn't touched yet, the use of \texttt{lst} as the second argument of \texttt{map}. When the refactoring swapped \texttt{map} for its difference list version it was able to determine the new type of both its arguments, \texttt{(a -> DList b)} for the first argument (the lambda expression) and \texttt{DList a} (\texttt{lst}). Fortunately because \texttt{lst} is a target of the refactoring its new type is \texttt{DList a} so this node can remain unchanged.


\begin{lstlisting}[caption={The final refactored result of \texttt{explode}.},label=explodeFinal]
import qualified Data.DList as DList
import Data.DList (DList)

explode :: DList a -> DList a
explode lst = DList.concat (DList.map (\x -> DList.replicate (length (DList.toList lst)) x) lst)
\end{lstlisting} 

The final result of this case of the Hughes list refactoring is in listing~\ref{explodeFinal}.

\section{Summary}

This chapter has introduced the concept of data-driven refactorings for the object-oriented programming paradigm and described three of the data-driven refactorings developed for this thesis. The first, introducing a type signature, creates additional abstraction to better describe what types are representing. Generalising maybe takes code written for a specific type and generalises it so that it is applicable to more types. Finally the list to Hughes list refactoring replaces one type with another equivalent type. This refactoring allows a project to be retyped mid-development because it's not always clear from the beginning of a project what the correct data representation should be.

The following chapters will expand on some of these ideas. The next chapter will describe another form of generalisation that rewrites monadic code into its equivalent applicative functor code. The applicative code can be much cleaner and descriptive in certain cases. It can also allow programmers to take advantage of a different way of executing their programs. Chapter~\ref{chap:monadification} discusses introducing effectful abstractions into pure code. This process helps a code base add additional features, such as shared state, mid-development.


\chapter{Generalising Monads to Applicative}
\label{applicative}

The previous chapter introduced the concept of a functional data refactoring and gave two examples, introducing a type synonym and generalising Maybe to MonadPlus. This chapter will cover another generalising refactoring in more depth, rewriting monadic functions to use applicative functors. 

In their 2008 functional pearl "Applicative programming with effects" Conor McBride and Ross Paterson introduced a new typeclass that they called Idioms but are also known as Applicative Functors~\citep{mcbrideIdioms}. Idioms provide a way to run effectful computations and collect them in some way. They are more expressive than functors but more general than Monads, further work was done in~\citep{arrowsAndIdioms} to prove that Idioms are also less powerful than Arrows.

Applicative functors were implemented in GHC as the typeclass \texttt{Applicative}. An interesting part of the history of GHC is that despite McBride and Paterson proving in their original functional pearl that all monads are also applicative functors, however,  GHC did not actually require instances of monad to also be instances of Applicative until GHC's 7.10.1 release~\citep{ghc7.10Release}. Now that every monad must also be an applicative functor there now exists a large amount of code which could be rewritten using the applicative operators rather than the monadic ones. 

This chapter will discuss the design and implementation of a refactoring which will automatically refactor code written in a monadic style to use the applicative operators instead. Section~\ref{sec:appOverview} is a brief overview of the \texttt{Applicative} typeclass's operators, section~\ref{sec:appProgStyle} will discuss the applicative programming style and, in general, how programs are constructed using the applicative operators, next, section~\ref{sec:appApps} will cover some common applications of this refactoring, section~\ref{sec:appRefact} will specify the refactoring itself, section~\ref{sec:appPrecons} covers the preconditions of the refactoring, finally section~\ref{sec:appVariations} outlines other refactorings that may be used in conjunction with the generalising monads to applicative refactoring and some possible variations of this refactoring. 

\section{The Applicative Typeclass}
\label{sec:appOverview}

The \texttt{Functor} typeclass defines a single function that must be implemented, \texttt{fmap}.

\begin{lstlisting}[frame=tblr]
class Functor f where
	fmap :: (a -> b) -> f a -> f b
\end{lstlisting}

The \texttt{fmap} function allows for a function to be applied to the contents of the Functor f. One could think of the functor as a context and \texttt{fmap} as a function that allows other functions to run within that context. However, what if you wanted to chain together sequences of commands within that context? This is not possible with just functors since \texttt{fmap} does not have the function inside of the functor's context. Sequencing commands will require a more powerful abstraction, applicative functors~\citep{realWorldHaskell}. 

In Haskell applicative functors are implemented in the \texttt{Applicative} typeclass. \texttt{Applicative} typeclass declares two functions, \texttt{pure} and \texttt{(<*>)}. The types of these two functions are shown in listing~\ref{appTypes} where \texttt{f} is the applicative functor. 

\begin{lstlisting}[frame=tblr,label=appTypes,caption={Types of Applicative's minimal complete definition}]
pure :: a -> f a
(<*>) :: f (a -> b) -> f a -> f b
\end{lstlisting}

The \texttt{pure} function is the equivalent of monad's \texttt{return}, it simply lifts a value into the applicative context. The other function \texttt{(<*>)} (which is typically pronounced "applied over" or just "apply"). Apply take in two arguments, both of which are applicative values. The first argument is function within an applicative context from types a to b, and the second argument is of type a. Apply returns a value of type b inside of the same functional context. Apply "extracts" the function from the first argument and the value from the second argument and applies it to the function, all within whatever the applicative context is.

\subsection{Other useful functions}

Though \texttt{pure} and apply are the only two functions that are required to be defined to declare an instance of applicative there are several other useful functions that can either be derived from these two functions or come from other typeclasses which will be briefly covered here. First there are two variations on apply.

\begin{lstlisting}[frame=tblr]
(*>) :: f a -> f b -> f b
(<*) :: f a -> f b -> f a
\end{lstlisting}

These functions sequence actions and still perform the contextual effects of both of their arguments but discard the value of the first and second argument respectively. These functions are used when some operation affects the applicative context but their returned value will not affect the final result of the applicative expression. For example when writing parsers it is common to have to consume some characters from the input without those characters affecting the final result of the parser.

A consequence of the applicative laws is that every applicative's functor instance will satisfy the following~\citep{control.applicative}: 

\begin{lstlisting}[frame=tblr]
f <$> x = pure f <*> x
\end{lstlisting}

The next section will cover how these functions can be used in an applicative style of programming. 

\section{The Applicative Programming Style}
\label{sec:appProgStyle}

In~\citep{mcbrideIdioms} the authors prove that any expression built from the applicative combinators can take the following canonical form:

\begin{lstlisting}[frame=tblr]
pure f <*> is_1 <*> ... <*> is_n
\end{lstlisting}


Where some of the \texttt{is}'s have the form \texttt{pure s} for a pure function \texttt{s}. Due to the rule mentioned at the end of the previous section this canonical form can also be expressed using the infix version of fmap \texttt{(<\$>)}. 

\begin{lstlisting}[frame=tblr]
f <$> is_1 <*> ... <*> is_n
\end{lstlisting}

This is the form that most programs will take when they are refactored from a monadic style. 


 Context-free parsing is a good use case of the applicative type and many examples in this chapter are taken from parsers defined using the parsec library~\citep{parsec}. The first example of the applicative programming style is a function that parses money amounts of the form \texttt{<currency symbol><whole currency amount>.<decimal amount>} e.g. "\$4.59" or "\textsterling64.56".
 
 \begin{lstlisting}[frame=tblr]
 data Currency = Dollar
                          | Pound
                          | Euro
              
data Money = M Currency Integer Integer

parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
 \end{lstlisting}
 
The \texttt{parseMoney} function is in the canonical form as defined by~\citep{mcbrideIdioms}. The pure function \texttt{M} is lifted into the \texttt{CharParser} context and its three arguments are provided by three smaller parsers that handle the currency symbol, the whole amount, and the decimal amount separately. 

The only difference between \texttt{readWhole} and \texttt{readDecimal} is that \texttt{readDecimal} has to consume the decimal point before reading the number. Instead of duplicating that number code let's perform a small refactoring to lift the parsing of the decimal into the \texttt{parseMoney} function which will allow us to reuse the \texttt{readWhole} function.

 \begin{lstlisting}[frame=tblr]
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <* char '.' <*> readWhole
 \end{lstlisting}
 
 Here we can see that the result of parsing the decimal point is discarded because of the use of \texttt{<*} rather than the full apply. All of the variations of apply are left associative so the following definition of \texttt{parseMoney} causes a type error.
 
  \begin{lstlisting}[frame=tblr]
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> char '.' *> readWhole
 \end{lstlisting}
 
This error can be corrected by wrapping "\texttt{char \textquotesingle.\textquotesingle~*> readWhole}" in parenthesis. 
 
The canonical style of applicative functions is not always the most idiomatic way to define things. The following function parses strings surrounded by double quotes.

\begin{lstlisting}[frame=tblr]
parseStr :: CharParser () String 
parseStr = char '"' *> (many1 (noneOf "\"")) <* char '"'
\end{lstlisting}

\texttt{parseStr} does not match the canonical form because no lifted pure function is applied to the rest of the applicative chain. This function could be transformed to canonical form by pre-pending "\texttt{id <\$>}."

The examples covered in this section give a basic introduction to programming in an applicative style. The next section will discuss common applications that are particularly well suited to definition in the applicative style and can be transformed from the monadic style. 

\section{Applications of the Refactoring}
\label{sec:appApps}

There are two things that make a particular application a good candidate for this refactoring. First, and most obviously, the application must be able to be defined using the applicative interface. Finally a good candidate will already have a large corpus of code that is already written in the monadic style. If a particular library already encourages its users to use it using applicative functors rather than monads then there is little work for the refactoring to do.

\subsection{Parsec}
Parser combinator libraries such as parsec~\citep{parsec} provide a simple way of building parsers from predefined smaller parsers (a.k.a. the combinators). The applicative interface is sufficient for defining parsers of context-free languages\footnote{This is mostly true. The applicative interface can parse context-sensitive languages if your grammar is an infinite size~\citep{appContextSens}.}. Despite this much of the code written using Parsec is monadic. A good example of this comes from Real World Haskell~\citep{realWorldHaskell}.

\begin{lstlisting}[frame=tblr]
csvFile :: GenParser Char st [[String]]
csvFile = 
    do result <- many line
       eof
       return result
\end{lstlisting}

This can be very simply rewritten using the applicative like so:

\begin{lstlisting}[frame=tblr]
csvFile :: GenParser Char st [[String]]
csvFile = many line <* eof 
\end{lstlisting}      
 
 Shorter code is not always better however, in this case,  I would argue that the applicative style is easy to clearer. The parser can be read left to right many lines are followed by the end of file.
 
\subsection{Yesod}
Another possible application of this refactoring applies to parts of code used to define Yesod webservers~\citep{yesod}. The preferred way to handle the creation and processing of web forms is using the applicative interface~\citep{yesodBook}. Yesod doesn't force forms to be handled applicatively because a monad instance is provided as well but it is the \textit{idiomatic} way to handle web forms. This refactoring would allow people to write in a monadic style and then refactor their code to fit the standard.

\subsection{Other applications}
\comment{Complete section conclusion here}
  

\section{Refactoring Monadic Programs to Applicative}
\label{sec:appRefact}
This section will cover the mechanics of refactoring monadic code to the applicative style. Many of these examples are taken from the parser for money amounts and a JSON parser. The full source of these parse can be found in~\citep{moneyParse} and~\citep{jsonParser}.

Take for example the following parser that parses strings that begin and end with double quotes.

\begin{lstlisting}[frame=tlrb]
parseStr :: CharParser() String
parseStr = do
	char '"'
	str <- many1 (noneOf "\"")
	char '"'
	return str
\end{lstlisting}

This parser first consumes a double quote (\texttt{char \textquotesingle"\textquotesingle}) then parses at least one other character other than double quotes and assigns those characters to the variable named \texttt{str}\footnote{This line is composed of two parser combinators, \texttt{many1}, and \texttt{noneOf}. \texttt{many1} takes another parser as its argument and applies it one or more times returning a list of the results. \texttt{noneOf} takes in a list of characters and succeeds if the current character is not in the provided list. Then the character is returned.}, finally the closing quote is consumed and \texttt{str} is returned. This particular function can be rewritten in an applicative style like so:

\begin{lstlisting}[frame=tlrb]
parseStr :: CharParser() String
parseStr = char '"' *> (many1 (noneOf "\"")) <* char '"'
\end{lstlisting}

The refactoring goes through the monadic version of the function line by line and composes computations with the applicative operators. In this case the first line of the \texttt{do} block does not affect the final result so it is followed by the \texttt{*>} which performs the action on the left hand side of the operator but discards that value. The next line's value is assigned to the variable str which is returned by the function so this means that on both sides of this computation the operator that composes it with its neighbours will have to "point" to it as well\footnote{This means \texttt{<*>} could occur on both sides, \texttt{*>} on the left, or \texttt{<*} on the right of the computation}. To determine which operator needs to be used on the right of the call to \texttt{many1} the refactoring needs to look at the next line and see if it also contributes a value to the final result. If it does then it and \texttt{many1} will be composed by the \texttt{<*>} operator, but since \texttt{char} doesn't, the operator between the call to \texttt{many1} and the second call to \texttt{char} becomes \texttt{<*} which discards the value \texttt{char} returns.

This is a fairly simple function to convert to applicative style. Let's look at another example that adds in more complexity by having multiple computations that contribute to the final value of the function. This function comes from the money parser that was used in~\ref{sec:appProgStyle} as well. 
\pagebreak
\begin{lstlisting}[frame=tlrb]
parseMoney :: CharParser () Money
parseMoney = do
   currency <- parseCurrency 
   whole <- many1 digit
   decimal <- (option "0" (do { 
                           char '.';
                           d <- many1 digit;
                           return d}))
   return $ M currency (read whole) (read decimal)
\end{lstlisting}

The \texttt{parseMoney} function parses text into the \texttt{Money} data type. It works by first consuming the currency symbol and getting the appropriate \texttt{Currency} type from that. Then the whole money amount is read from one or more digits. Finally an "option" parser attempts to match a decimal point followed by one or more digits. If that fails then the decimal amount is zero. These three values are then combined into type \texttt{Money} which is returned. \texttt{parseMoney} can be rewritten in an applicative style like so\footnote{The where clause in this example has been included for formatting and readability and would not be generated automatically.}:

\begin{lstlisting}[frame=tlrb]
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
          where readWhole = read <$> many1 digit
                  readDecimal = read <$> option "0" (do { 
                           char '.';
                           d <- many1 digit;
                           return d}))
\end{lstlisting}

On the left hand side of the chain of applicative actions there is a call to a pure operation, in this case the constructor \texttt{M}. Any pure computations that "collect" the values returned from applicative computations will appear on the left of the chain of operations. The pure computations are composed with applicative computations with the \texttt{<\$>} operator which lifts the computation into the applicative context. The three computations are composed together with the \texttt{<*>} operator because they all contribute to the final result of \texttt{parseMoney}.

The chains of applicative computations can get more complicated. The following snippet of code parses a single in a JSON object which consists of a string key and a value which can be any valid JSON value, separated by a colon, and stores the key and value in a tuple. 

\begin{lstlisting}[frame=tblr]
objEntry = (,) <$> (spaces *> parseStr <* spaces <* char ':' <* spaces) <*> (parseJVal <* spaces)
\end{lstlisting}

When there are a large amount of computations that do not affect the final value of the function as a whole there can be multiple valid ways the chain can be structured. The \texttt{objEntry} function can be defined in several different ways as shown below.

\begin{lstlisting}[frame=tblr]
objEntry = (,) <$> (spaces *> parseStr <* spaces <* char ':') <*> (spaces *> parseJVal <* spaces)

objEntry = (,) <$> (spaces *> parseStr) <*> (spaces *> char ':' *> spaces *> parseJVal <* spaces)
\end{lstlisting}

Both of the above versions of \texttt{objEntry} are equivalent to the first version. The automated refactoring will produce the first version of \texttt{objEntry}. The refactoring in general will produce applicative chains according to the following rules. Both sides of the apply operator will be parenthesised statements. After the first value producing operation every side effect causing operation will be composed with \texttt{(<*)}. In general the produced applicative chain will take the following form.

\begin{lstlisting}[frame=tblr]
f = pf <$> (is_1 *> ... is_n *> vs_1 <* js_1 ... <* js_n) <*> (vs_2 <* ks_1 ... <* ks_n) ... <*> (vs_n ...)
\end{lstlisting}

The \texttt{vs} are functions that return values to be passed to the pure function \texttt{pf} all other functions just run within the applicative context without affecting the returned value of \texttt{f}.

\section{Preconditions of the Refactoring (When is a Monad actually a Monad?)}
\label{sec:appPrecons}

Many functional refactorings have non-trivial preconditions that must hold before the refactoring can be applied~\citep{refacTools}. Fortunately this refactoring only has a single fairly simple precondition, the function to be refactored must be definable with the applicative interface not just the monadic interface. What does this mean exactly? Where is the line between applicative and monadic? Let's start by looking at the type signatures of the bind and apply functions.

\begin{lstlisting}[frame=tblr]
(<*>) :: Applicative f => f (a -> b) -> f a -> f b

(>>=) :: Monad m => m a -> (a -> m b) -> m b
\end{lstlisting}  

One thing to keep in mind is that these two functions' arguments are in opposite order. The key difference becomes clearer when examining the type of apply when its arguments are flipped.

\begin{lstlisting}[frame=tblr]
flip (<*>) :: Applicative f => f a -> f (a -> b) -> f b
\end{lstlisting}

The only difference between this version of apply and bind is in the second argument. Bind takes in a function that takes in a value of type \texttt{a} and returns an \texttt{m b} whereas apply receives an applicative functor that contains a function from \texttt{a} to \texttt{b}. This means that within a monadic context bind allows access to the pure value contained in the monad while all of the arguments to apply are contained within the applicative functor.

What does this mean in practice? For example, the following functions are taken from a StackOverflow answer by Conor McBride~\citep{soApp}.

\begin{lstlisting}[frame=tblr]
iffy :: Applicative a => a Bool -> a x -> a x -> a x
iffy ab at af = pure cond <*> ab <*> at <*> af where
  cond b t f = if b then t else f

miffy :: Monad m => m Bool -> m x -> m x -> m x
miffy mb mt mf = do
  b <- mb
  if b then mt else mf
\end{lstlisting} 

Both of these functions attempt to implement an if statement, \texttt{iffy} does it with \texttt{pure} and \texttt{(<*>)} whereas \texttt{miffy} uses the monadic functions. Both functions' first argument contains a boolean within its computational context. Depending on the value of this boolean the second or third argument is then returned. Though both of these functions will return the same value when given the same arguments, the effects within the context will be very different. When \texttt{iffy} is run all of the contextual effects will be run regardless of which value is returned.

If a value retrieved from the monadic context is used in a right hand side expression before the return statement then that function cannot be refactored to use the applicative interface without changing the contextual effects of the function. 

\section{Variations and Related Refactorings}
\label{sec:appVariations}

This section will discuss related refactorings and variations on the generalise monad to applicative refactoring that may be useful. Related refactorings help transform code so that it can pass the preconditions. In this case it may be possible to extract monadic code into another function making the top level function definable with the applicative interface. Variations on refactorings slightly change the behavior of the refactoring. This section will present two variations, one which will inline small do blocks of monadic code automatically, another will
recursively refactor do blocks that may occur inside of higher level statements.

\subsection{Extract monadic code}
\label{subSec:extract}
Say someone wanted to refactor the following code to use the applicative interface rather than the monadic one it currently uses.

\begin{lstlisting}[frame=tblr]
f = do
	x <- getX
	b <- getB
	y <- if b then getY1 else getY2
	log y
	return (x,y)	
\end{lstlisting}

This code will not pass the precondition because both \texttt{b} and \texttt{y} are used on the right as well as the left hand side of the equation. However, lines three through five don't really affect the rest of the function so they could be refactored into their own function then f could be rewritten applicatively.

\begin{lstlisting}[frame=tblr]
f = (,) <$> getX <*> g

g = do
	b <- getB
	y <- if b then getY1 else getY2
	log y
	return y
\end{lstlisting}

\subsection{Inline do blocks}
Instead of extracting an entire function as in subsection~\ref{subSec:extract} a developer may prefer to just inline a do block. This is useful if the monadic section is fairly small.

\begin{lstlisting}[frame=tblr]
f = do
	x <- result1
	y <- result2
	z <- result3
	log z
	return (x,y)
\end{lstlisting}
\larger[5]
\[\Rightarrow\]
\normalsize
\begin{lstlisting}[frame=tblr]
f = (,) <*> result1 <*> (result2 <* do{z <- result3; log z})
\end{lstlisting}

Normally the variable \texttt{z} would prevent the function from being refactored. Introducing the small do block allows for a simple readable applicative function to be produced. 

It is worth noting that if the variable \texttt{z} was also included in the output of the function the do block inlining would still work with a slight modification.

\begin{lstlisting}[frame=tblr]
f = (,) <*> result1 <*> result2 <*> do{z <- result3; log z; return z}
\end{lstlisting}

\subsection{Refactor Inner \texttt{do} blocks}
If we take another look at the \texttt{parseMoney} function and it's applicative counterpart the default behaviour of the refactoring preserves the inner do block passed to the \texttt{option} parser.

\begin{lstlisting}[frame=tlrb]
parseMoney :: CharParser () Money
parseMoney = do
   currency <- parseCurrency 
   whole <- many1 digit
   decimal <- (option "0" (do { 
                           char '.';
                           d <- many1 digit;
                           return d}))
   return $ M currency (read whole) (read decimal)
   
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
          where readWhole = read <$> many1 digit
                  readDecimal = read <$> option "0" (do { 
                           						char '.';
                          						d <- many1 digit;
                           						return d}))
\end{lstlisting}

It is perfectly possible to refactor this inner do block to the applicative style as well.

\begin{lstlisting}[frame=tblr]

parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
          where readWhole = read <$> many1 digit
                  readDecimal = read <$> option "0" (char '.' *> many1 digit)
\end{lstlisting}


\chapter{Introducing Effectful Abstractions}\label{chap:monadification}
Up to this point the data refactorings that have been discussed are changing abstractions that already existed in the source code. This chapter will explore refactorings that introduce effectful abstractions into pure code. In particular this chapter will focus on introducing monads and applicatives into pure code.

The \texttt{Identity} monad is the monad the does not embody any computation strategy~\citep{identityMonad}. This means that any pure Haskell function could be refactored to be within the Identity monad. This refactoring can take in a set of functions and produce a corresponding set of functions with a monadic type. Take for example this definition of the Fibonacci numbers.

\begin{lstlisting}[frame=tblr]
fib :: Int -> [Int]
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
\end{lstlisting} 

This can be refactored to the Identity monad like so:\pagebreak

\begin{lstlisting}[frame=tblr]
fib :: Int -> Identity [Int]
fib 0 = return 0
fib 1 = return 1
fib n = do
	x <- fib (n-1)
	y <- fib (n-2)
	return (x + y)
\end{lstlisting}

The new code could then be rewritten very easily (just by changing the type signature) to be within another monad. This allows for a developer to quickly create programs that can take advantage of monadic features such as IO or state. The monadic code could also be generalised to use applicatives with the refactoring detailed in chapter~\ref{applicative} but I also hope to develop a more straightforward way to introduce applicatives for this chapter.

Finally another abstraction that would seem to have quite a bit of potential for automatic introduction is the \texttt{Arrow} typeclass. Arrows were originally introduced as a more general abstraction to monads in~\citep{genMonadsArrows}. The full relationship between arrows, monads, and applicative functors was more fully described in~\citep{arrowsAndIdioms}. Given the relationship between these three typeclasses I believe it will be worth exploring introducing arrows as well. A possible outline of this chapter would be:

\begin{enumerate}
\item Introducing the Applicative style
\item Automated Monadification
\item Introduction to Arrows
\item Discussion of Arrows, Applicative, and Monads
\item Refactoring to Arrows
\end{enumerate} 

\if(FALSE)
This chapter will cover the refactoring of pure code to become monadic. This chapter will be structured much like chapter~\ref{applicative} with sections covering the motivation behind the refactoring, examples of the refactoring, the preconditions that must hold before applying the refactoring, and variations and related refactorings. 

The \texttt{Identity} monad is the monad the does not embody any computation strategy~\citep{identityMonad}. This means that any pure Haskell function could be refactored to be within the Identity monad. This refactoring can take in a set of functions and produce a corresponding set of functions with a monadic type. Take for example this definition of the Fibonacci numbers.

\begin{lstlisting}[frame=tblr]
fib :: Int -> [Int]
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
\end{lstlisting} 

This can be refactored to the Identity monad like so:

\begin{lstlisting}[frame=tblr]
fib :: Int -> Identity [Int]
fib 0 = return 0
fib 1 = return 1
fib n = do
	x <- fib (n-1)
	y <- fib (n-2)
	return (x + y)
\end{lstlisting}

The new code could then be rewritten very easily (just by changing the type signature) to be within another monad. This allows for a developer to quickly create programs that can take advantage of monadic features such as IO or state.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


This chapter will cover the refactoring pure code to be monadic instead. Take for example the following standard definition of mergesort.

\begin{lstlisting}[frame=tblr]
mergesort :: (Ord a) => [a] -> [a]
mergesort [] = []
mergesort [x] = [x]
mergesort xs = merge (mergesort fst) (mergesort snd)
  where fst = take (length xs `div` 2) xs
        snd = drop (length xs `div` 2) xs

merge :: (Ord a) => [a] -> [a] -> [a]
merge xs [] = xs
merge [] ys = ys
merge (x:xs) (y:ys)
   | (x <= y) = x:(merge xs (y:ys))
   | otherwise = y:(merge (x:xs) ys)
\end{lstlisting}

Say that one wanted to refactor this code to become monadic. There are two different ways this could be interpreted, either both \texttt{mergesort} and \texttt{merge} are rewritten to be monadic, or just \texttt{mergesort} could be refactored. \texttt{Merge} cannot be refactored to be monadic without also changing \texttt{mergesort} because \texttt{mergesort} is dependent on the result of calls to \texttt{merge}. If \texttt{merge}'s result is monadic any client functions will also have to be monadic because there is no way to remove the value from a monad. 

Refactoring \texttt{mergesort} to be monadic without refactoring \texttt{merge} produces the following code.

\begin{lstlisting}[frame=tblr]
mergesort :: (Ord a) => [a] -> Identity [a]
mergesort [] = return []
mergesort [x] = return [x]
mergesort xs = do
  x <- mergesort fst
  y <- mergesort snd
  return $ merge x y
    where fst = take (length xs `div` 2) xs
             snd = drop (length xs `div` 2) xs
\end{lstlisting}  

Any program can be refactored into the \texttt{Identity} monad as this monad does not express any computational strategy~\citep{identityMonad}. Here we can see that in cases where pure values would have been returned from \texttt{mergesort} pre-refactoring have now been prepended with \texttt{return}, this lifts these values into the \texttt{Identity} monad. The recursive calls to \texttt{mergesort} now need to be bound to variables in the do block which allow the pure results to be passed to calls to the still pure \texttt{merge}.

Instead of refactoring just \texttt{mergesort} refactoring \texttt{merge} as well would produce slightly different code.

\begin{lstlisting}[frame = tblr]
merge :: (Ord a) => [a] -> [a] -> Identity [a]
merge xs [] = return xs
merge [] ys = return ys
merge (x:xs) (y:ys)
   | (x <= y) = do
       ls <- merge xs (y:ys)
       return (x:ls)
   | otherwise = do
       ls <- merge (x:xs) ys
       return (y:ls)

mergesort :: (Ord a) => [a] -> Identity [a]
mergesort [] = return []
mergesort [x] = return [x]
mergesort xs = do
  x <- mergesort fst
  y <- mergesort snd
  merge x y
    where fst = take (length xs `div` 2) xs
            snd = drop (length xs `div` 2) xs
\end{lstlisting}

The above example's definition of \texttt{mergesort} is very similar to the previous definition. The only difference is that previously the call to \texttt{merge} was a pure value so it needed to be lifted into the monad by \texttt{return}, in this version since merge was also refactored into the monad it can just sit at the end of mergesort.

\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related work}

There are several bodies of literature that are related to my thesis work. Other functional refactoring tools such as Wrangler~\citep{wrangler} are of obvious interest. There is also the code smell tool for Haskell HLint~\citep{hlint}.

Another interesting project is the Type-and-Transform system developed at the University of Utrecht~\citep{typeAndTransform}. Which is a system for performing semantics preserving type changing transformations for the simply typed lambda calculus, and the polymorphic lambda calculus.

In general this chapter will briefly discuss the research done in two primary areas:

\begin{enumerate}
\item Refactoring and code smell tools for functional programming languages
\item Type changing program transformations 
\end{enumerate}

\section{Tooling for functional languages}

\begin{enumerate}
\item HLint
\item Wrangler
\item Other refactoring tools from Budapest
\item (From above) http://haskelltools.org/
\item The budapest people also did a refactoring tool for wrangler
\item OCaml refactorer?
\end{enumerate}


\section{Type changing program transformations}

\begin{enumerate}
\item Type and transform
\item Erwig and Ran monadification
\item Claus Reinke monadification
\end{enumerate}

\comment{Move applicative do stuff here?}

\section{Program transformation and Refactoring}

This section covers some of the relevant previous work done in the fields of refactoring and program transformation. It should be said that there is much more work done in program transformation than is discussed here. More general discussion about the field of program transformaton in general can be found in~\citep{visserSurvey,transformationIntro}.

\subsection{Type and transform systems}

Work on a theoretical type changing program transformation system is being done currently at Utrecht University. They have proposed a type changing program transformation system known as a "type-and-transform" system~\citep{typeAndTransform}. This type-and-transform system can transform standard Haskell lists to Hughes' lists and perform stream fusion~\citep{typeAndTransform}. The type-and-transform system was expanded in~\citep{typeAndTransformPatterns} to properly transform Haskel lists to the $Seq~a$ type found in the \textit{Data.Sequence} module. At this point the type-and-transform system only works on a subset of the Haskell language~\citep{typeAndTransform}

\subsection{Automatic Monadification}\label{erwigMonad}

Another type of program transformation that many people are interested in involves the "monadification" of code. Monads are necessary for Haskell code to perform any operation that would typically cause side-effects (I/O, state, etc.). This means that a system that can transform code without monads to code with monads, would be highly desirable to the Haskell community.

Erwig and Ran have proposed just such a system in~\citep{monadification}. Their system can be broken down into three steps. \textit{Navigating} involves finding the expression that will be returned by the function. Next \textit{binding} identifies recursive calls and replace the with variable binding. Finally the return is applied to the expression \textit{navigating} identified in the \textit{wrapping} step.

\subsubsection{Issues with Erwig and Ran's solution}

There are a few issue with the monadification solution proposed by~\citep{monadification}. There several different styles of monadification and this algorithm only supports one of them~\citep{clausMonadResponse}. Five different styles of monadification have been outlined in~\citep{monadSurvey}, and before implementing an automatic monadification solution it should be determined which style of monadification would be the most helpful to the Haskell community.

\subsection{Functional language refactoring tools}

Refactoring is a subtopic of standard program transformation in that the goal of refactoring is to leave program behavior unchanged~\citep{fowler}. There are quite a few functional language refactoring tools that already exist for Haskell and other functional languages. 

\subsubsection{Erlang}

Erlang is another functional programming language that puts a heavy emphasis on fault tolerance, scalability, and concurrency. In contrast to Haskell, Erlang is not statically typed.
 
Several projects exist to improve the quality of Erlang code. RefactErl is a source code analysis tool that also supports a few refactorings~\citep{refactErlWiki}. Tidier is an automated Erlang refactorer. Unlike many of the tools discussed in this paper Tidier runs without user input, cleaning up Erlang source code automatically~\citep{erlangTidier}.

Wrangler is another refactoring tool for Erlang made by Huiqing Li and Simon Thompson here at the University of Kent~\citep{refacTools}. In addition to refactorings Wrangler also provides "decision support tools" that detect code smells and highlight other issues within an Erlang project. Wrangler is also user extensible, it supports a domain-specific language allowing users to script their own complex refactorings~\citep{refacTools}. Wrangler has been integrated into the Emacs and Eclipse IDEs.

\subsubsection{Haskell}

Improving the quality of Haskell code is the focus of several projects. HLint for example provides suggestions for how Haskell code could be improved but does not do any actual transformations~\citep{hlint}.

Another Haskell program transformation project is HERMIT. Instead of performing transformations on Haskell source code HERMIT targets the Core language~\citep{hermit}. Core is the intermediate language that Haskell source code is transformed to after desugaring~\citep{ghcDesign}. HERMIT uses GHC plugins to perform program transformations as compiler optimisation passes. 

Finally there is the Haskell Refactorer (HaRe). Since HaRe is where I will be implementing my research I have dedicated an entire chapter (chapter ~\ref{hareChapter}) to describing its implementation. 

\comment{\textbf{Add section about ML languages.}}

\chapter{Conclusion}
Summarise my contribution here. The main contributions of my thesis are the development of type changing refactorings for GHC. With a particular emphasis on changing the abstractions that programs use.

\bibliography{main}

% This index section is optional, use cleardoublepage and phantomsection to make the links work in your contents page. Uses makeidx package.
\cleardoublepage
\phantomsection
\label{index}
\printindex

\end{document}
