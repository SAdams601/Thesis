% Documentation:
%
% The UoK class file extends the standard report style to follow the Registry
% guidelines for laying out a thesis. It sets the margins, interline spacing,
% the page, figure and table numbering style, and disallows page breaks at
% hyphens. The class file consists of setting one and an half line spacing text
% with a 4cm left margin, at least a 2.5cm right margin, approximately 2cm top
% and bottom margin, on A4 paper.
% 
% The class the following options, in addition to those of the standard report
% class.
%     mini - Toggles the thesis in to mini-thesis mode. This adds "mini" to the
%            title and appends a nocite(*) at the end for an automatic output of
%            your complete bibliography.
%     draftmark - Puts a DRAFT' watermark on every page of the document along
%                 with the draft statement on the title page. Additionaly, it
%                 is used as a switch for the UoKExtentions package.
%     draft - Puts the entire document into draft mode. Applies all the effect
%             of draftmark above, but also propergates to other packages used.
%     copyright - Adds a copyright page between the title page and the preface.
%     nofig - Disables output of the list of figures in the preface.
%     notab - Disables output of the list of tables in the preface.
% All options passed to UoKthesis will be passed along to included packages:
%    natbib, draftwatermark, setspace, hyperref, lmodern
%
% The cover page and optional copyright page are implicitly added before the
% start of the preface section. Use the following commands to populate the 
% cover page/copyright page information:
%     \title{thesis title}
%     \author{author's name} 
%     \degree{Master of Science, Doctor of Philosophy, etc.} 
%     \subject{author's department}
%          - Computer Science if omitted 
%     \submitdate{month year in which submitted}
%          - dated by LaTeX if omitted 
%     \copyrightyear{year degree conferred (next year if submitted in Dec.)}
%          - assumes current year (or next year, in December) if omitted 
% 
% The preface environment allows for the use of sections that precede the main
% document; such as Abstract and  Acknowlegements. These sections should be
% defined using \section{Preface Section Title}. The contents page (and list of
% figures and tables if in use) will be automatically inserted at the end of the
% preface environment.
%
% The thesis style invokes the setspace package to set the commands:
%     \doublespace
%     \onehalfspace
%     \singlespace
% for spacing. By default one and an half spacing is used which resembles the
% UKC Typewriter requirement. Singlespace can be used for letterpress
% appearance. If you want to use true double space, for some reason, place the
% \doublespace command where you want to start using double spacing. Just call
% the appropriate spacing command at where you want to use them.
% 
% In the figure and table environments, single spacing is used. If you want to
% use any other size rather than one and an half spacing, then do:
% 	\renewcommand{\baselinestretch}{1.6} (or whatever you want instead of 1.6)
% This command won't take effect unless it comes before the \begin{document} or
% is triggered by a font change (after something like \small \normalsize).
%
% The example below shows the 12pt thesis style being used. This seems to give
% acceptable looking results, but it may be omitted to get 10pt. Alternatively,
% the 11pt option can be used.
%
% This version differs from old_ukcthesis.sty in the following ways:
% 1. Removed the doublespace package (now uses setspace).
% 2. Merged the phantom section for correct PDF links into the bibliography
%    generating function. 
% 3. Added thesis type options (mini, draft).
% 4. Kent Harvard is used for referencing and citation, this is supported by the
%    natbib package.
% 5. PsFig macro removed.
% 6. Now comes as two files, UoKthesis.cls, which defines purely stylistic layout,
%    and UoKextentions.sty, that provideds some additional functionality.

\documentclass[12pt]{UoKthesis}

%\renewcommand*\rmdefault{ptm}
%\renewcommand{\familydefault}{\rmdefault}
% Note: The UoKextentions package includes the xcolor package with the [usenames]
% options. If you need to add further options, these can be given to UoKextentions
% to be propogated through.
\usepackage{UoKextentions}
\usepackage{times}
%\usepackage{llncsdoc}
%\usepackage{verbatim}
\usepackage{url}
\usepackage{color}

\usepackage{amsmath}
\usepackage{relsize}
\usepackage[final]{listings}
\usepackage[T1]{fontenc}
%\usepackage[math]{times}
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\newcommand{\td}[1]{{\bf {\tt{#1}}}}
\newcommand{\comment}[1]{\textcolor{red}{\td{{#1}}}}
\usepackage{textcomp}
\usepackage{csquotes}
\lstset{
  frame=none,
  xleftmargin=2pt,
  stepnumber=1,
  numbers=left,
  numbersep=5pt,
  numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  stringstyle=\mdseries\ttfamily,
  showspaces=false,
  keywordstyle=\bfseries\ttfamily,
  columns=flexible,
  upquote=true,
  showstringspaces=false,
  basicstyle=\small\ttfamily,
  breaklines=true,
  morecomment=[l]\%,
}

% Kent Harvard Bibliography Style. WIP
\bibliographystyle{kentHarvard}

% Provides nice linking in PDFs
\usepackage{hyperref}

% Only needed if you want to produce an index. Example is shown at the bottom of this document.
\usepackage{makeidx}

% Useful packages
% \usepackage{epstopdf} % Converts EPS files to PDF using ghostscript
% \usepackage{fnbreak}  % Warns you if you have split footnotes
% \usepackage{mathpazo} % Type­set­ math­e­mat­ics in the Palatino fam­ily of text fonts
% \usepackage{paralist} % Enumerate and itemize within paragraphs
% \usepackage{amsmath}  % AMS mathematical facilities
% \usepackage{rotating} % Rotating facilities for floats


\setcounter{secnumdepth}{3} % add more section types

%%%%% macros
\def\fixme#1{\fbox{\textbf{\textsc{Fixme}}\quad#1}}
\def\fixpic#1{\fbox{\textbf{\textsc{Picture}}\quad#1}}
\def\defnx#1#2{\emph{#1}\index{#2}}
\def\defn#1{\defnx{#1}{#1}}
\def\floatpic#1#2{%
\begin{wrapfigure}{r}{\dimexpr #1 / 2 \relax}
\includegraphics[width=\dimexpr #1 / 2 \relax]{#2}
\end{wrapfigure}}
\def\inlinepic#1#2{%
\begin{center}
\includegraphics[width=\dimexpr #1 / 2 \relax]{#2}
\end{center}}

%%%%% augment hyphenation
\hyphenation{wide-spread}

%%%%% document start
\begin{document}

\title{Data-Driven Refactorings for Haskell}
\author{Stephen Adams}
\subject{Computer Science}
\degree{PhD}

\begin{preface}
\section{Abstract}

Refactoring is the process of changing the internal structure of a program without changing its external behaviour. Refactoring increases code quality and reduces its technical debt. However, refactoring by hand is time consuming and error-prone. This makes automated refactoring tools very useful.

Agile software development allows for software to evolve slowly over time. This evolution changes how a program processes, abstracts over, and views its data. This evolution, though necessary, comes with the cost of technical debt. As technical debt increases changes to a code base become more difficult. Refactoring is one of the primary ways to  reduce technical debt. 

There exist refactorings that specifically help software to evolve its data model, however these refactorings are specific to the object-oriented programming paradigm. Haskell is a strongly typed, pure functional programming language. Haskell's rich type system allows for complex and powerful data models and abstractions. This thesis reports on work done to design and automate refactorings that help Haskell programmers develop these abstractions.

This work also discussed the current design and implementation of HaRe (the \textit{Ha}skell \textit{Re}factorer). HaRe now supports the Glasgow Haskell Compiler's implementation of the Haskell 2010 standard and its extensions. 

\section{Acknowledgements}
I would like to thank...
\end{preface}

\chapter{Introduction}\label{chp:intro}


\section{Functional Programming}
Functional programming is a programming paradigm that focuses on data values as described by expressions which are built from function applications and definitions~\citep{elementsOfFunc}.  Functions in this case are closely related to the idea of mathematical functions. Another key concept of a functional programming language is that functions are \textit{first class citizens}. This means that functions can be used like any other type of data, e.g. as arguments to other functions. 
  
\section{Haskell}
Haskell is a statically typed, lazily evaluated, pure functional language. Haskell is strongly and statically typed, and supports Hindley-Milner type inference~\citep{wikiIntro}. Type inference means that a Haskell programs do not need every type to be explicitly listed in the source code. Types will be~\textit{inferred} at compilation time so that every part of a Haskell program's type is known at that time. Haskell's type system also allows users to define their own types.

Lazy evaluation means that Haskell expressions are not evaluated when they are passes as a parameter, but rather when that value is used. For example in the Haskell function:
\clearpage
\begin{verbatim}
f x y = case x > 0 of
True -> x - 1
False -> x + 1
\end{verbatim}

The parameter y will never be evaluated.

Haskell is also a pure language. Purity is the idea that functions cannot perform actions in addition to returning values. These additional actions are known as side-effects. Haskell allows for traditionally side-effect causing operations (IO, state, etc.) through the use of monads. Monads contain any potentially side effect causing operations inside of them allowing the rest of the program to remain pure.

\section{Refactoring} 
Refactoring is the process of changing a program without changing its behaviour. This is done to improve its internal structure~\citep{fowler}. Behaviour preservation is what separates refactoring from other types of program manipulation. This idea of functionality preservation means that refactoring will not introduce new bugs or eliminate old ones. To prevent semantic changes after refactoring, many refactorings have non-trivial preconditions~\citep{refacTools}. 

Manual refactoring is tedious and error prone, and depends on high testing coverage to ensure that functionality is preserved~\citep{fowler}. This means that tools that can automatically perform refactorings and ensure that preconditions are met are highly desirable.

\subsection{Functional refactoring}

Refactoring a functional language has a few key differences from refactoring an imperative language. The higher-order nature functional languages means that any sub-expression of a function is a candidate for generalisation whereas in other languages the types of parameters and results is limited. The semantics of functional languages also allow for the complete checking of preconditions based on the static semantics of the language~\citep{refacTools}.

It is also not unusual for functional refactorings to be substantially different than their object-oriented (OO) counterparts. For example creating a case statement from a multi-equation function definition in a functional language versus inlining a virtual method as a case statement in an OO language require substantially different program manipulations~\citep{huiqingThesis}. Additionally there can be refactorings with no OO counterpart, monadification, for instance.  

\section{Contributions of this Research}

\comment{TODO}

\section{Justification and limitations of tooling}

A refactoring tool can only do so much the user needs to be "sufficiently smart" \comment{elaborate on this}

For tools to be successful they just have to bridge the gap between the original code to a version of the code that the programmer is willing to modify to what they want. Without the tool the transformation would be too risky or monotonous for the user to attempt alone.

  

\section{Thesis Outline}

\comment{TODO}


\chapter{Background: Refactoring Haskell in HaRe}
\label{hare}

\comment{
Chapter~\ref{hare} is where the development and implementation of HaRe will be discussed. The chapter with cover some of the history or HaRe and the briefly the technology that it was originally developed with. Next it will cover the design and implementation of HaRe currently and its dependencies (in particular ghc-exactprint). }

\td{
A brief outline of this chapter:

\begin{enumerate}
\item The original implementation of HaRe
\item The GHC API
\item Generic Programming (Scrap Your Boilerplate and Strafunski-StrategyLib)
\item GHC-ExactPrint
\item The Current version of HaRe
\end{enumerate}
}

Work on HaRe started in early 2003 at the University of Kent. HaRe was originally created by Huiqing Li, Claus Reinke, and Simon Thompson~\citep{refacWebsite}. This chapter begins with a brief discussion of the original implementation of HaRe. HaRe was originally implemented to support the Haskell 98 standard~\citep{huiqingThesis}. The Haskell ecosystem has evolved a great deal since then. Haskell 2010 is the current formal language standard but the Glasgow Haskell Compiler (GHC) has become the de facto Haskell standard~\citep{refacTools}. GHC supports the entire 2010 standard but also includes language extensions that do everything from changing the type system to adding new syntax features~\citep{langExts}.



\section{The original implementation of HaRe}

Implementing an automated refactoring system has several dependencies, a frontend for the language that you are targeting, a generic programming library, and a pretty printer. The language frontend is required for the refactorer to analyse and modify source code, the generic programming library assists in traversing the complex abstract syntax tree of a real world programming language, and the pretty printer outputs the modified AST in a form recognizable by the author of the original program. The original implementation of HaRe fulfilled these dependencies with two libraries, Programatica and Strafunski-StrategyLib. 

\subsection{Programatica and Strafunski}\label{prog&Strafunski}

Programatica was a project at the OGI School of Science and Engineering to build tool support for validating Haskell programs~\citep{programaticaTools}. The Programatica team open sourced their frontend so that other tools could also use it~\citep{refacWebsite}. The HaRe team chose to use Programatica over other available front ends (such as GHC's internal front end) because it was the simplest front end that supported the full Haskell 98 standard along with a number of its extensions~\citep{huiqingThesis}. Programatica's Haskell front end is broken up into multiple components including a lexer, a parser, an abstract syntax, a module system, a type checker, and a pretty printer. Programatica's frontend allows for HaRe to focus on refactoring only rather than having to build all of these components as well.

Programatica's abstract syntax contains 20 data types with 110 data constructors in total. Working with the syntax directly would introduce a large amount of "boilerplate" code into HaRe that would make maintenance and reusability of much more difficult~\citep{huiqingThesis}. Instead HaRe used Strafunski-StrategyLib, a combinator library for generic programming, to traverse the abstract syntax tree (AST) of the source code~\citep{strafunski}. 

Commonly transforming a program only modifies small sections of the source program's AST. Renaming a function, for example, will only need to modify the name used in the binding and places where that name is being used; all other sections of the AST remain unmodified. A commonly used operation in Strafunski takes a function that works on a particular data type (or types)\footnote{In the renaming example this could be code that checks if a variable is the one being renamed and replacing it with the new name.} and extends it to work on all types by leaving all other types unmodified. Strafunski also provides "strategies" that define how that extended function will be applied to the syntax tree~\citep{strafunski}\footnote{e.g. top-down or bottom-up}. The using Strafunski will be discussed in more detail in section~\ref{genProg}.

These two dependencies allowed the original implementation of HaRe to obtain the abstract syntax tree of source code to be refactored, traverse and transform that AST, and output the modified program. All these tasks are things that the current implementation of HaRe needs to do but the dependencies that it relies on have changed somewhat. The Haskell standard was updated in 2010~\citep{haskell2010} and GHC continues to expand the number of language extensions it supports. Unfortunately Programatica has not kept up with these changes and does not support anything beyond Haskell 98. At the same time GHC's own API has matured so that it can replace Programatica as HaRe's front end. HaRe now relies on this and a few other projects for its language frontend. For generic traversals "Scrap Your Boilerplate" has been added as a dependency though Strafunski-StrategyLib is still used~\citep{syb}.
 
\subsection{HaRe's original refactorings}\label{origRefactorings}

HaRe's original refactorings fall into three categories, structural, module, and data-oriented refactorings.

\subsubsection{Structural Refactorings}

Structural refactorings principally concern the name and scope of entities defined in a program~\citep{huiqingThesis}. These refactorings modify functions, and smaller sections of code. A traditional example of this is the renaming refactoring. 

Renaming is one of the most basic refactorings. The purpose of renaming is to change the name of a given identifier. A renaming refactoring could target a variable, function name, type, or any other piece of syntax that a programmer can name. This allows for the way in which you refer to your code to easily stay up to date with what your code actually does. 

Other examples of structural refactorings include deleting a definition, duplicating a function, and adding an argument~\citep{huiqingThesis}.

\subsubsection{Module Refactorings}

Module refactorings concern the imports and exports of an individual module, or the relocation of definitions between modules~\citep{huiqingThesis}. A simple refactoring in this category would be "clean an import list," which analyses a module's import list and removes redundant import declarations. Another example of a refactoring in this category would be the "move a definition" refactoring. As its name suggests, move a definition takes a definition from one module and moves it to another and fixes the imports and exports of any affected modules. For example if we were moving some function \texttt{foo} from module \texttt{A} to module \texttt{B} any external dependencies of \texttt{foo} need to be imported into \texttt{B} and if those dependencies are no longer used in \texttt{A} the import relevant statements should be removed. If \texttt{foo} is still being used in \texttt{A} then \texttt{A} needs to import \texttt{B} (if it does not already do so). Finally any other modules that currently depend on \texttt{foo} need to now import \texttt{B} (if it's not already imported) and remove the import of \texttt{A} (assuming none of \texttt{A}'s other definitions are used).

\subsubsection{Data Type Based Refactorings}

The third category of refactorings are those that are associated with data type definitions~\citep{huiqingThesis}. "Add field names" is a good example of a data-oriented refactoring.  The add field names refactoring will add field names to a data type. These names can then be used as selector functions that make extraction of a particular part of a type a simple function call. The new field names are generated by HaRe but can be renamed by the user~\citep{huiqingThesis}.

The adding of field names alone seems like an unusual 
\comment{Add description of concrete to abstract data type refactoring. Also use it to introduce composite refactoring. See: https://www.cs.kent.ac.uk/projects/refactor-fp/catalogue/ConcreteToAbstract.html and huiqings thesis}

These original refactorings were chosen to be basic yet still useful, and for their ability to give insight into the issues surrounding implementing an automated refactoring tool~\citep{huiqingThesis}. In addition to the refactorings that were implemented by HaRe's developers an API was exposed so that other developers could implement their own refactorings~\citep{hareApi}.


\subsection{The HaRe API}\label{hareApi}

Early in the development cycle of HaRe it was restructured to expose an API for implementing refactorings and general Haskell program transformations~\citep{hareApi}. The HaRe API contains a collection of functions for program analysis and transformation of Haskell 98 programs. These functions along with the functionality provided by Strafunski and Programatica form the basis for implementing basic refactorings~\citep{hareApi}.

The HaRe API exposes the full Programatica abstract syntax for Haskell 98 to the user but because of generic programming with Strafunski only the to be transformed parts of the AST have to be explicitly referenced in a refactoring~\citep{hareApi}. Another key feature of the API was to hide layout and comment preservation allowing the programmer to focus on program transformation instead. Each subtree in the AST is tagged with its absolute location in the source file. Any modifications to the AST will change the location of all elements that occur after the change. The API abstracts over this cascade of changes that follows even the simplest of changes. The HaRe API transformation functions modify the token stream and the AST simultaneously which keeps refactoring definitions free of this location bookkeeping ~\citep{hareApi}. 

The overall goal of the HaRe API is to help ensure the correctness of new refactorings by limiting the amount of code required that is not related to program transformation, and isolating common error sources~\citep{hareApi}. This design goal still guides the development of HaRe, and many of the functions that were provided in the original HaRe API have been updated to HaRe's latest implementation. 
 
\section{Underlying technologies}

As was mentioned in Section~\ref{prog&Strafunski}, HaRe's dependencies have changed somewhat in its current implementation. HaRe originally used Programatica as its front end, now the language front end is composed of several projects now, the GHC API, ghc-mod, and ghc-exactprint~\citep{ghcApi}, \citep{ghcMod}, and \citep{exactprint}. This section will describe these dependencies as well as Scrap Your Boilerplate~\citep{syb}, another generic programming library, and how HaRe currently uses them.

\subsection{The GHC API}

Rather than being a monolithic executable, the Glasgow Haskell Compiler (GHC) is composed of several smaller components that each correspond to a separate compiler stage. The GHC executable consists of a lightweight main function that ties together the smaller components~\citep{ghcDesign}. These components are exposed to users  and this is what constitutes the GHC API.

\subsubsection{Compiler stages of GHC}\label{ghcStages}

Some of the major components and the order that GHC uses them are shown in figure~\ref{compilerStages}. This figure is not a complete list of all the components GHC uses just the parts that HaRe interacts with. The full diagram can be found in~\citep{ghcDesign}. The label after each compiler stage indicates the type of AST that is produced by that stage.

\begin{wrapfigure}{L}{0.5\textwidth}\label{compilerStages}
	\begin{center}
		\includegraphics[scale=.3]{images/compilerStages.png}
	\end{center}
	\caption{GHC Compiler stages. Adapted from \citep{ghcDesign}}
\end{wrapfigure}

The top level datatype for all of the GHC abstract syntax is \texttt{HsSyn}~\citep{ghcDesign}. \texttt{HsSyn} is parameterised by some identifier; each compiler stage produces a different type of identifier with the additional information that stage produces. For example the typechecker takes in an AST parameterised by \texttt{Name} and returns an AST parameterised by \texttt{Id} which is a \texttt{Name} with additional type information.

\subsubsection{GHC's Name Types}

There are five name types that GHC uses, they are:

\begin{itemize}
	\item \texttt{OccName}
	\item \texttt{RdrName}
	\item \texttt{Name}
	\item \texttt{Id}
	\item \texttt{Var}
\end{itemize}

\texttt{OccName} is the simplest type of name it is really just a wrapper around a \texttt{FastString} and an optional \texttt{NameSpace}. It isn't produced by any the other four name types, rather an \texttt{OccName} is contained in each of the other names. An \texttt{RdrName} is the type that parameterises the parsed abstract  syntax. \texttt{RdrName}s aren't much more than an \texttt{OccName} and possibly a module name that indicates what module the name is defined in.

The renamer produces an AST that uses \texttt{Name}s. \texttt{Name}s form the basis for the rest of the GHC's identifiers. A \texttt{Name} wraps \texttt{OccName} along with what type of name it is (user-defined or system-defined), where that \texttt{Name} is defined, and a unique integer so that names with the same \texttt{OccName} can be differentiated. 

The other two name types, \texttt{Id} and \texttt{Var}, are actually synonyms.

\begin{lstlisting}[frame=single,caption={Id's definition from Var.hs in~\cite{ghcHackage}}]
type Id = Var
\end{lstlisting}

These types are essentially just typed \texttt{Name}s. The difference between \texttt{Id}s and \texttt{Var}s is that instead of a type \texttt{Vars} have a kind, and \texttt{Id}s contain more information about what sort of syntax element the \texttt{Id} refers to. 

All these different types of names change throughout the compilation process but these names only parameterises the syntax tree, the shape~\comment{shape? form?} of the tree itself stays the same through compilation. 

\subsubsection{GHC's syntax tree}
GHC's abstract syntax is currently made up of over 90 data types. 

\comment{How do I describe the AST? Do I want to go into much detail? It seems like this could help motivate the need for generic programming if done correctly.}

\section{Generic programming}\label{genProg}
The need for a generic programming library, as previously discussed in section~\ref{prog&Strafunski}, remains the same when using the GHC API's AST as opposed to Programatica's. Currently HaRe still uses Strafunski-StrategyLib as well as another library, Scrap Your Boilerplate. Scrap Your Boilerplate (SYB) is a generic programming library developed by Ralf L{\"a}mmel and Simon Peyton-Jones~\citep{syb}.
\subsection{Generic Traversals}
The Stratego/XT library was one of first transformation systems~\citep{stratego}. Stratego developed the idea of a transformation strategy. A transformation strategy is the combination of a term rewriting function and how that rewriting will be applied to the tree of terms. Stratego provides combinators from which term rewriting functions can be constructed and tree traversal functions\citep{stratego}.

Stratego is an untyped transformation system and so was unsuitable for working with the statically typed Haskell.  
\subsection{Scrap Your Boilerplate}\label{syb}

	Suppose there was a simple expression language that contained integers, integer addition, assignment, and variables. This language is represented by the type defined below.
	
	\begin{verbatim}
type Name = String

data Expr =
     Value Int
   | Var Name
   | Add Expr Expr
   | Assign Name Expr
      deriving(Data,Typeable)
	\end{verbatim}
	
	If you wanted to define a function that renamed the variable called "x" to be called "a" it would look something like this:
	
	\begin{verbatim}
renameXVar :: Expr -> Expr
renameXVar (Var "x") = Var "a"
renameXVar (Assign c e) = 
	| c == "x" = Assign "a" (renameXVar e)
	| otherwise = Assign c (renameXVar e)
renameXVar (Add e1 e2) = Add (renameXVar e1) (renameXVar e2)
renameXVar v = v
	\end{verbatim}
	
	This is fairly straight forward and doesn't take much time to write. However, if subtraction was added to the definition of expression \texttt{renameXVar} would need to be updated as well to include a recursive call very similar to the addition case. These duplicated recursive calls are what's known as "boilerplate" code~\citep{syb}. Boilerplate code is highly repetitive, verbose, and difficult to .  In this small example having a few of these types of cases is not an issue. However, as the expression begins to approach the size of an actual programming language writing traversals like \texttt{renameXVar} become much more time-consuming and a nightmare to maintain. 
	
	The reduction of boilerplate code like this is the point of SYB. SYB allows us to rewrite \texttt{renameXVar} as:
	
	\begin{verbatim}
import Data.Generics	
	
rename :: Name -> Name
rename "x" = "a"
rename n = n

renameXVar :: Expr -> Expr
renameXVar = everywhere (mkT rename)
	\end{verbatim} 
	
	This example nicely illustrates the four key components of an SYB traversal~\citep{syb}.

	\begin{itemize}
		\item The function that performs the "interesting" part of the traversal
		\item A type extension for that function
		\item A generic traversal combinator
		\item The data type to be traversed must be an instance of the Typeable and Data classes
	\end{itemize}
	
	From the example mentioned previously the "interesting" part of this traversal is the rename function because this function contains the code that actually changes the name "x" to the name "a." The \texttt{mkT} function extends the type of the \texttt{rename} function to \texttt{Typeable~a = > a~ -> a}. 
	
	Type extension allows for the \texttt{rename} function to work over any members of the \texttt{Typeable} class rather than just \texttt{Name}s. The extended version of \texttt{rename} will work as expected when provided with an argument of type \texttt{Name}, and will act as the identity function if an argument of any other type is provided.
	
	The \texttt{everywhere} function is this traversal's generic combinator, \texttt{everywhere} applies a generic function to every node in the tree. Finally as you can see in the declaration of \texttt{Expr} derives both the Typeable and Data classes so it can be traversed by \texttt{everywhere}. A member of the Typeable class has defined a generic representation of itself and members of the Data class implement generic folding operations. Put together these two classes are what allow a data type to be generically traversed. 
	
\subsubsection{Types of Generic Algorithms}

SYB defines three types of generic algorithms, transformations, queries, and monadic transformations. The \texttt{rename} example from the previous section is an example of a transformation. Transformations preserve the type of the structure that is traversed. Queries, on the other hand, are type unifying algorithms. Queries are good for summarizing information contained in a data structure. You would use a query, for example, to traverse an expression and collect all of it's the bound variables.

It is also useful a lot of the time to do transformations from within some monadic context. This type of generic traversal is very common in HaRe because traversals often need to make use of a refactoring's stored state or run something from the GHC API which needs the features provided by a \texttt{GhcMonad}.  
	 
\section{ghc-exactprint}

After using generic programming to transform the parsed abstract syntax, HaRe needs to be able to print the modified code. A challenging part of building a refactoring tool is that a user wont want non refactored parts of their code to change at all. HaRe needs to preserve the user's comments and spacing from the source file.

Prior to GHC version 7.10.1 the location of certain keywords and punctuation (such as \texttt{do} and \texttt{let}) and user comments were lost after parsing. This made parsing and then printing an exact copy of a GHC Haskell file impossible. GHC's 7.10.1 release added annotations of the lost syntax elements that had previously not been represented in the parsed abstract syntax~\citep{apiAnns}. 

The parser produces a map that associates the keyword and the source span that the keyword can be found in with that keywords exact location. This approach was taken to avoid littering the existing AST with functionally meaningless keyword data~\citep{apiAnns}. 

Even with every syntax element's position being recorded, printing a module after the AST has been modified is not easy. Many AST elements are "located" with a source span that indicates that element's exact position in the file. This means that any change to the AST will require updating the location of all the syntax elements that occur later in the line at least for single line changes and, in the case of changes that modify entire lines, every element after that change will need its location to be updated.

Ghc-exactprint simplifies this immensely by allowing us to position elements relative to their neighbours rather than absolutely~\citep{exactprint}. After parsing a source file HaRe takes the annotations GHC returns with the parsed abstract syntax and relativise them using ghc-exactprint. For each syntax element ghc-exactprint creates a new data type called an "\texttt{Annotation}" which contains an offset that indicates where this element should be positioned compared to the previous element. Take for example the following definition.

\begin{lstlisting} 
f a = (a+ 1 )
\end{lstlisting}

The absolute position of the plus sign is row one column nine (GHC's locations are one-based) but using ghc-exactprint we instead can think of the position of the plus sign using the offset \texttt{(0,0)} because there is no space between it and the previous element (the variable \texttt{a}). Using this system the number literal following the plus sign has an offset of (0,1) because of the single column of space before it. 

The \texttt{Annotation}s are stored in a map that is keyed based on the parsed location of a syntax element and the string representation of the AST constructor. In the previous example the right hand side of the definition is located at the source span (1,7)-(1,13) which stands for row one columns seven through thirteen and GHC represents this expression with the \texttt{HsPar} which is a constructor for the \texttt{HsExpr} type. The source span and the constructor together can be combined to retrieve the annotation data associated with this bit of the AST. This syntax tree has two elements associated with it, the opening and closing parenthesis. Each of these keywords is given its own offset, which in this case is (0,0) for the opening parenthesis\footnote{The offset in this case is (0,0) and not (0,1) as you might expect because the space between the equals sign and the opening parenthesis is represented in the offset for the entire right hand side expression.} and (0,1) for the closing parenthesis. We could obviously infer from the use of the \texttt{HsPar} constructor that this tree is wrapped in parenthesis however the users specific spacing would be lost without the annotations.  

\section{The current implementation of HaRe}

This section has previously discussed the major dependencies that HaRe uses. The ghc-api give us access to the internal representation of Haskell, the generic programming libraries Scrap Your Boilerplate and Strafunski-StrategyLib allows HaRe to more easily work with that internal representation, and ghc-exactprint is how HaRe preserves the source files spacing when writing the output file. From this foundation we can make HaRe much more focused on refactoring rather than solving these more generic problems. This section will discuss how HaRe is implemented, what its API provides, and some general conventions that it's refactorings use.

\subsection{HaRe's inner workings}\label{hareInners}

The GHC API work within the \texttt{GhcMonad} that provides the features GHC needs to compile a single Haskell source file such as IO, logging warning, exception handling, and keeping track of the compilation session~\citep{ghcApi}. In reality Haskell programs consist of more than just single source files without dependencies. Projects are organised using build tools such as Cabal or Stack~\citep{cabal,stack} that handle these issues for the programmer. HaRe needs to be aware of the context that these tools provide because refactorings may change multiple modules or modify modules that import modules from external dependencies. In HaRe's case ghc-mod provides a monadic context that handles these build environments and compiler setup~\citep{ghcMod}. Within ghc-mod's context HaRe keeps track of the state of the refactoring session.

\begin{lstlisting}[caption={HaRe's Monad \texttt{RefactGhc}},captionpos=b, label=refactghc] 
newtype RefactGhc a = RefactGhc
    { unRefactGhc :: GM.GhcModT (StateT RefactState IO) a}
\end{lstlisting}

Listing~\ref{refactghc} shows the definition of HaRe's monad that each refactoring runs in. \texttt{RefactState} is an ADT that keeps track of all the settings, abstract syntax, and filepath for refactoring a single file. When refactoring client modules you can just set the filepath in the \texttt{RefactState} to target another module and HaRe will parse and typecheck that module's information into the state.

Even though there are three types of the AST the annotations are part of the parsed AST so ghc-exactprint works with the parsed AST only. Once a refactoring has finished it is expected that the parsed source and annotations will reflect all the changes that the refactoring made. HaRe's state still contains all three of the syntax trees because the renamed and typed source are useful for the additional information they contain about the source file. 

\subsection{HaRe's API}

Modifying and reasoning about GHC's abstract syntax and maintaining the associated annotations is still a complex task even with help from HaRe's dependencies. HaRe defines its own API to help fill this gap between its dependencies and its refactorings. In addition to the obvious functions that are required for running a refactoring within the \texttt{RefactGhc} monad the API also includes helper functions that make working with the state easier. 

HaRe also defines a large collection of program analysis and transformation functions. For example, pulling the binding of a top level variable from a module's entire abstract syntax tree is a task that many refactorings have to do so this functionality is part of HaRe's API. There are also small program transformations that are not in and of themselves refactorings but common low level modifications that are useful to several refactorings, such as adding a new import declaration or making a function infix by wrapping it in back quotes (the \texttt{`} character).

Additionally there are several transformations that don't affect the abstract syntax as much as they change the annotations that format ghc-exactprint's output. Adding new lines before a syntax element doesn't change the meaning of a program but is important for a refactoring's output to be well formatted and easy to read. 

HaRe's dependencies help abstract away the low level\footnote{Low level from the perspective of a refactoring at least.} details of a language back-end, build tools, and pretty printing. HaRe's API tries to close the gap still left between the dependencies and the refactorings themselves. In the next section we will take a look at how HaRe's refactorings are implemented.

\subsection{Implementing Refactorings in HaRe}

HaRe actually requires very few things from a refactoring implementation. As was mentioned previously in section~\ref{hareInners} everything must run inside of the \texttt{RefactGhc} monad who's state is where the abstract syntax of a target module is stored. A refactoring is also expected to return a list of  "\texttt{ApplyRefacResult}s" which contains an updated parsed AST and annotations along with the filepath the AST originated from; this updated AST is what HaRe writes out as the result of the refactoring\footnote{HaRe actually outputs to a temporary file. When refactoring "file.hs" HaRe produces "file.refactored.hs." This allows programmers to check the result of a refactoring before overwriting the existing module.}  


Beyond those two features refactorings are free to be implemented however the programmer chooses. However, certain conventions have been adopted within many of HaRe's refactorings. When describing a refactoring one would imagine that checking \textit{pre}conditions would be the first thing the implementation of that refactoring computes. A more efficient implementation checks preconditions throughout computation alongside the AST transformation. Merging precondition checking with transformation saves the refactoring from traversing parts of the AST multiple times, for example, the renaming refactoring checks for name conflicts while it descends the AST replacing the old name with the new one. Obviously this strategy only works for certain preconditions the only precondition for deleting a definition is that the target definition isn't used. The transformation only affects the syntax tree of the definition to be deleted so the implementation of the refactoring has to do a separate scan of the rest of the target module and any of its client modules to determine if the target definition is used or not. 

\chapter{Data-driven refactorings}

\comment{Is this too introductory? Should I move this to the first chapter?}

On the first day of the first programming course of my Bachelors degree my professor tasked us with defining what a "program" is. In the end the classes conclusion was that a program is a process that computes a result from some input. The input and output are the "data" that a program operates over. Through my education I've found that understanding the relationship between input and output is key to writing a program.

The data a program works over highly influences its structure and definition. However one major challenge that programmers face is that there are still multiple correct solutions for many problems though many of those solutions may be suboptimal. On top of this challenge a programmers understanding of the problem could change or the requirements could evolve as the project develops. All of this means that the first implementation of a program is rarely optimal and if it is, it rarely remains optimal as time goes on. 

Data-driven or data-oriented refactorings for functional languages have been discussed in the literature for some time. Notable examples include work on monadification(\cite{monadification},\cite{monadSurvey}) and transforming datatypes~(\cite{datatypeTransformation}, \cite{brownThesis}).  The goal of this thesis is to refine this class of refactorings.

This chapter will introduce what I mean by "data-driven" refactorings. The chapter begins with discussion of how these object-oriented refactorings translate (or don't) to a functional language. From there the chapter will give several examples of data-driven refactorings for Haskell.

\section{Object-Oriented Data Refactorings}\label{ooRefs}

The origins of refactoring are deeply rooted in the object-oriented world~(\cite{programRestructuring},~\cite{refactOOFrameworks}). The canonical catalogue of refactorings remains Martin Fowler's \underline{Refactoring: Improving the Design of Existing Code} ~\citep{fowler}. Fowler's catalogue of refactorings are all written in Java though he purposefully avoided using any features that were unique to Java so that the refactorings could be useful in many different programming languages.

As a functional programmer when going through this catalogue of refactorings there seem to be three types of refactorings.

\begin{itemize}
	\item Refactorings that are applicable in a functional language
	\item Refactorings not applicable to a functional language
	\item Refactorings that could be adapted for use in a functional language
\end{itemize}

That first type of refactorings' usefulness to a functional program is easily understood. Refactorings like renaming or adding a parameter don't depend on object-oriented features of the target language. 

The second type of refactorings are so dependent on features associated with object-oriented languages that they are impossible to implement or are meaningless in a functional language. The "remove setting method" refactoring depends on the common OO pattern of each field of a class having "getter" and "setter" methods that retrieve or modify that field respectively. This pattern, on the other hand, is not as ubiquitous in functional languages because many do not support objects and immutability makes a "setter" function not in the functional style. OCaml would be a notable exception to this rule. OCaml supports objects and allows the programmer to mark variables as mutable so getter and setter methods are possible.

\begin{lstlisting}[caption={An OCaml object with getter and setter methods.},captionpos=b, language=caml, morekeywords={object,method},label=ocamlObj]
let mInt init_i = object
    val mutable i = init_i

    method get_i = i
    method set_i new_i =
      i <- new_i
  end
\end{lstlisting}

This OCaml object from Listing~\ref{ocamlObj} would be a valid target for this refactoring. However these methods are not as ubiquitous as they are in imperative object-oriented languages with mutable data as the default so a refactoring to remove a setter method is of limited value even for OCaml.

The third type of refactoring found in~\citep{fowler} is much more interesting to a functional programmer. The specifics of these refactorings aren't directly applicable to functional programs but the underlying motivations are relevant to any programming paradigm. 

For example here's the motivation for the "Replace Data Value with Object" refactoring in~\citep[pg. 175]{fowler}:

\begin{displayquote}
Often in early stages of development you make decisions about representing simple facts as simple data items. As development proceeds you realize that those simple items aren't so simple anymore. A telephone number may be represented as a string for a while, but later you realize that the telephone needs special behavior for formatting, extracting the area code, and the like. For one or two items you may put methods in the owning object, but quickly the code smells of duplication and feature envy. When the smell begins, turn the data value into an object.
\end{displayquote}

This refactoring extracts a field that was some primitive type into an object. The example from~\citep{fowler} works over an order class with a string that representing the customer that placed the order.

\begin{lstlisting}[caption={The Order class}, language = java, captionpos=b,tabsize=4]
class Order {
	public Order (String customer) {
		_customer = customer;	
	}
	
	public String getCustomer() {
		return _customer;
	}
	
	public void setCustomer(String arg){
		_customer = arg;	
	}
	
	private String _customer;
}
\end{lstlisting}

The refactoring creates a new customer class that just has a string field with a getter method as seen in Listing~\ref{custCls}. The customer class doesn't add any additional features but the extra layer of abstraction sets up the code base for further development. The customer object could have fields added that represent contact info or further demographic information without polluting the order class with non order relevant data.

\begin{lstlisting}[caption={The result of the Replace Data Value with Object refactoring when applied to the customer field of the order class.}, language = java, captionpos=b,label=custCls,tabsize=4]
class Order {
	public Order (String customer) {
		_customer = new Customer(customer);	
	}
	
	public String getCustomer() {
		return _customer.getName();
	}
	
	public void setCustomer(String arg){
		_customer = new Customer(arg);	
	}
	
	private Customer _customer;
}

class Customer {
	public Customer(String name){
		_name = name;
	}
	
	public String getName() {
		return _name;
	}
	
	private final String _name;
}
\end{lstlisting}

Functional programmers have to make similar data representation decisions as the object-oriented programmer. At the start of a project representing a customer just by their name could be reasonable. As the project develops this can become a serious limitation and a more robust abstraction is required.

This section introduced how object-oriented refactorings help build up the data model over a projects lifetime. In an object-oriented language the primary abstraction method is to just introduce additional objects. These sorts of refactorings for a language with a rich type system like  Haskell offers many more choices for evolving a system's data model. The rest of this chapter will describe refactorings for Haskell that support this evolution.  

\section{Data-Driven Refactorings in Haskell}

Haskell offers a rich environment for data representation. The Haskell 2010 standard defines several types that will be included in the prelude including tuples, lists, characters, strings (which are just lists of characters), several types of numbers, and the function type. Additionally programmers can construct new types with algebraic data types or rename an existing type with type synonyms. Type classes supports overloading as well. The standard library of GHC comes with many type classes that can help produce powerful abstractions~\citep{typeclassopedia}. Contrast this with how most object-oriented type systems are either unified where every type is a subclass of some top level \texttt{Object} class (e.g. C\# or Ruby) or there are a set of predefined primitive types which cannot be changed as well as the object hierarchy (e.g. C++ and Java's type systems). 

These two different approaches to type systems both have pros and cons which has sparked a vigorous (and quite possibly eternal) debate. The goal of this thesis is not to add to this debate, instead hopefully a few things are clear at this point.

\begin{itemize}
	\item Data representation is a language independent problem that must be answered in every project.
	\item The way a project manages and represents its data will (and should) evolve over a project's lifetime.
	\item Refactoring is a structured way to support this evolution.
	\item Refactorings for a language like Haskell need to take a different approach than those for object-oriented languages.
\end{itemize} 

\subsection{Introducing a Type Synonym}\label{introSyn}

Type synonyms in Haskell, as mentioned previously, are a way to name an existing type. A simple example can be seen in listing~\ref{fooSyn}. 

\begin{lstlisting}[label=fooSyn,caption={A simple type synonym.}]
type Foo = (String, Int)

f :: Foo -> Foo
f x@(_, 0) = x
f (str, i) = (tail str, i-1) 
\end{lstlisting}

Any place that where the \texttt{Foo} synonym is in scope the new name can be used to refer to any value of type \texttt{(String, Int)}. In fact "\texttt{Foo}s" and "\texttt{(String, Int)}s" are completely interchangeable. Introducing a synonym is a good way to quickly and simply name types to suggest their specific use in the current application.

Returning to the example from~\citep{fowler} used in section~\ref{ooRefs} of an order type that keeps track of the customer who placed the order (among other things presumably but they have been left out here for brevity's sake), see listing~\ref{haskellOrder} for a Haskell implementation of this type and a function that counts how many orders a particular customer has placed in a list of orders.

\begin{lstlisting}[label=haskellOrder,caption={An order algebraic data type.}]
data Order = Order {customer :: String}

numberOfOrdersFor :: [Order] -> String -> Int
numberOfOrdersFor orders name = length (filter (\ord -> name == (customer ord)) orders)
\end{lstlisting}

The current representation of a customer as just a \texttt{String} is a bit underdeveloped as it is. Introducing a customer synonym is a simple step that sets up the code base for further development. The synonym will help clearly mark which strings in the program stand for customers and which are  not.

The introduce a type synonym refactoring works by taking a type and a valid synonym name (as per the Haskel 2010 standard~\citep{haskell2010}) and creates a new synonym. In this case the type is \texttt{String} and the synonym name should be something like "\texttt{Customer}." The only precondition of the refactoring is that the new synonym name cannot cause a name clash. 

\begin{lstlisting}[caption={The customer synonym}]
type Customer = String
\end{lstlisting}

After this the next part of the refactoring involves replacing the appropriate uses of the type with the new synonym. This part of the refactoring is difficult to automate and needs to be interactive\footnote{This is not a feature of HaRe yet.}. There is no way to infer which instances of \texttt{String} (in the case of this example) should be replaced with the \texttt{Customer} synonym. The code from listing~\ref{haskellOrder} can have all of the string instances replaced by \texttt{Customer} because all the strings are being used to represent one. If there were a second function \texttt{printThankYou} which has type \texttt{String -> Order -> IO ()} which prints out a customized thank you message to the customer from the business for their order. 

\begin{lstlisting}[caption={The printThankYou function}]
printThankYou :: String -> Order -> IO ()
printThankYou businessName order = do
	putStrLn ("Thank you " ++ (customer order) ++ " for your order.")
	putStrLn (businessName ++ " hopes to see you again soon!")
\end{lstlisting}

Though the first argument to \texttt{printThankYou} is a \texttt{String} it does not represent a customer therefore shouldn't be replaced by the new synonym. The implicit meaning of the first argument hasn't been encoded in the type system so the programmer has to intervene during the refactoring to make their intention for each instance of \texttt{String} clear. The final result of the refactoring can be seen in listing~\ref{orderRefact}. 

\begin{lstlisting}[label=orderRefact,caption={The final result after adding a synonym for String.}]
type Customer = String

data Order = Order {customer :: Customer}

numberOfOrdersFor :: [Order] -> Customer -> Int
numberOfOrdersFor orders name = length (filter (\ord -> name == (customer ord)) orders)

printThankYou :: String -> Order -> IO ()
printThankYou businessName order = do
	putStrLn ("Thank you " ++ (customer order) ++ " for your order.")
	putStrLn (businessName ++ " hopes to see you again soon!")
\end{lstlisting}

 This transformation might seem like too small of a step. Wouldn't it be preferable to introduce a more powerful abstraction such as an algebraic data type? One of the principles of the Agile Manifesto is "simplicity" which is described as "maximizing the amount of work not done is essential"~\cite{agileManifesto}. The work not done in this case is the introduction of a more complex customer representation. This small step does clearly differentiate the strings that represent customers from other strings that represent other types of data.


\section{Maybe to MonadPlus}
 
A common data refactoring is generalisation. Generalisation is taking code written for a specific type and rewriting it to use a more general type. The newly generic code is applicable in more places and can help reduce code duplication.\underline{Refactoring} dedicates an entire chapter to generalisation cataloguing refactorings like "extract subclass" and "extract interface"~\citep[pg. 319]{fowler}. 

Generalisations for object-oriented languages either move functionality up in a hierarchy (as in "push method up") or change the hierarchy by adding classes (e.g. "extract subclass"). Object-oriented languages have a single hierarchy since every object inherits from a root class typically just called \texttt{Object}. Functional languages don't have this single unified hierarchy, but smaller hierarchies do exist because type classes can inherit from one another. 

This thesis will outline two generalisations, one makes code of a specific type work over a type class instead (this section), the other generalises code of one type class to use a class higher the hierarchy(covered in chapter~\ref{applicative}). This section describes a refactoring that rewrites programs of type \texttt{Maybe a} to use \texttt{MonadPlus m => m a}. 

\begin{lstlisting}[caption={The \texttt{Maybe} data type definition and \texttt{MonadPlus} instance declaration, and the \texttt{MonadPlus} class definition.},label=maybeMonadPlus]
data Maybe a = Nothing
                 | Just a
                 
instance MonadPlus Maybe where
   mzero = Nothing
   Nothing `mplus` r = r
   l          `mplus` _ = l
                 
class Monad m => MonadPlus m where
   mzero :: m a
   mplus :: m a -> m a -> m a
\end{lstlisting}
 
Listing~\ref{maybeMonadPlus} contains the \texttt{Maybe} instance of and the class declaration for \texttt{MonadPlus}. The \texttt{Maybe} type represents a computation (returned wrapped in the \texttt{Just} constructor) that can fail (represented by the \texttt{Nothing} constructor). The \texttt{MonadPlus} class is a typeclass for monads that also have a monoidal structure~\footnote{A monoid is a semigroup who's associative binary operation has an identity element.}. 

The \texttt{MonadPlus} class helps generalise monads that contain some concept of failure and choice. The \texttt{mzero} value represents a failed computation and \texttt{mplus} represents a way of making a "choice" between two computations that may or may not have failed~\citep{typeclassopedia}. 

\begin{lstlisting}[caption={\texttt{Maybe}'s monad definition}, label=maybeMonad]
class Applicative m => Monad m where
	return :: a -> m a
    (>>=)  :: forall a b. m a -> (a -> m b) -> m b
    
instance Monad Maybe where
	return = Just
	
	(Just a) >>= f = f a
	Nothing  >>= _ = Nothing
\end{lstlisting}

This refactoring replaces \texttt{Maybe} specific code with the more general \texttt{Monad} and \texttt{MonadPlus} operations. This is done by recognizing when \texttt{Maybe} specific code is also replaceable by a more general operation. Listing~\ref{mmp1} contains a simple example of this.  

\begin{lstlisting}[caption={},label=mmp1]
inc :: Maybe Int -> Maybe Int
inc Nothing = Nothing
inc (Just i) = (Just (i + 1))
\end{lstlisting}

The function \texttt{inc} can be rewritten to use bind (\texttt{>>=}) instead, this is because \texttt{inc}'s definition matches \texttt{Maybe}'s definiton of bind. When \texttt{inc} is called with \texttt{Nothing} it returns \texttt{Nothing}, otherwise it modifies the value inside of the \texttt{Maybe}. This matches the behaviour of bind so \texttt{f} can be rewritten as seen in listing~\ref{mmp1Ref}.

\begin{lstlisting}[caption={},label=mmp1Ref]
inc :: Maybe Int -> Maybe Int
inc mi = mi >>= (\i -> (return (i+1)))
\end{lstlisting}
 
The first example could be rewritten using only functionality provided by the \texttt{Monad} type class. Obviously this isn't always the case as seen in listing~\ref{mmp2}.

\begin{lstlisting}[caption={}, label=mmp2]
showNat :: Int -> Maybe String
showNat i =
  if (i <= 0)
    then (Just (show i))
    else Nothing
\end{lstlisting}

The function \texttt{showNat}, instead of taking in a \texttt{Maybe} type, takes in a pure value and returns a \texttt{Maybe} based on that value. A general version of \texttt{showNat} needs to be able to express the idea of failure that \texttt{MonadPlus} provides.

\begin{lstlisting}[caption={}, label=mmp2Ref]
showNat :: (MonadPlus m) => Int -> m String
showNat i =
  if (i <= 0)
    then (return (show i))
    else mzero
\end{lstlisting}
 
Listing~\ref{mmp2Ref} shows the refactored version of \texttt{showNat}. This time the refactoring can only replace the \texttt{Maybe} constructor calls with the more generic \texttt{Monad} and \texttt{MonadPlus} values. 
 
\section{Lists to Hughes Lists a Refactoring}

The previous section discussed a refactoring that is most useful in the early stages of development when the details of data representation are in their infancy. As the project develops more and more decisions must be made about how data is stored and processed. Mid-development it may become necessary to change what data structure the program is used.

This section will cover a refactoring for automatically replacing a type by "projecting" that type into another. The two types (the original and the new type) don't necessarily to have a formal relationship from the compiler's point of view (e.g. both implement the same type class) but certain properties should hold. 

\subsection{Hughes Lists}

Appending two lists into a single list is a basic and key operation. The standard implementation of append is seen in listing~\ref{concat}. 

\begin{lstlisting}[caption={The standard definition of concat},label=concat]
(++) :: [a] -> [a] -> [a]
[]  ++ ys = ys
(x:xs) ++ ys = x:(xs ++ ys)
\end{lstlisting}

If the first argument to append is the empty list then append can just return the second argument. In the other case append traverses the first list popping off the head of the first list and recursively appending the tail of the first list and the second argument. The performance of append is proportional to the length of its first argument. This has the unfortunate side effect where if a program builds of a list by repeatedly appending  to the end of a list the program will spend significant amounts of time traversing the beginning of the list over and over. In total the performance of this function ends up being $O(n^2)$ where $n$ is the length of the final list.

Fortunately there is an alternative representative of lists that allows $O(n)$ time appends. This alternative representation was first described by John Hughes in~\citep{hughesList} (hence their name), they are also known as difference lists~\citep{realWorldHaskell}; difference lists is the name that they are provided by in Hackage~\citep{dlist}. In Hughes lists elements are stored as partial applications of the append function, these partial applications are then composed together using function composition (the \texttt{(.)} operator in Haskell). 

Difference lists store the values \texttt{[1,2,3]} as \texttt{([1,2,3] ++)} which is of type \texttt{Num a => [a] -> [a]}. Appending \texttt{[4,5,6]} to \texttt{([1,2,3] ++)} first involves converting it to a difference lists (\texttt{([4,5,6] ++)} in this case) then these two difference lists can be appended with function composition which results in \texttt{([1,2,3] ++) . ([4,5,6] ++)}. 

\begin{lstlisting}[caption={Building and deconstructing difference lists.},label=ghciDList]
> let lst = ([1,2,3] ++) . ([4,5,6] ++)
> :t lst
lst :: Num a => [a] -> [a]
> lst []
[1,2,3,4,5,6]
\end{lstlisting}

As seen in listing~\ref{ghciDList}, once it comes time to retrieve the normal list from a difference list applying an empty list to the difference list and the partial applications are evaluated from right to left. Internally difference lists are just a wrapper around a function from lists to lists. Listing~\ref{dlistDef} shows the definition of the \texttt{DList} new type which contains the partial application. The \texttt{unDL} function simply removes the \texttt{DL} constructor. 

\begin{lstlisting}[caption={The definition of \texttt{DList} taken from~\citep{realWorldHaskell}}, label=dlistDef]

newtype DList a = DL {
   unDL :: [a] -> [a]
}

fromList :: [a] -> DList a
fromList xs = DL (xs ++)

toList :: DList a -> [a]
toList (DL xs) = xs []

append :: DList a -> DList a -> DList a
append xs ys = DL (unDL xs . unDL ys)
\end{lstlisting}

While difference lists support fast appends there is no such thing as a free lunch, speed ups for certain functions are paid for by slowdowns in other places. Getting the head and tail of a normal list are both constant time operations but become linear time for difference lists because the difference lists will have to be converted back to normal lists. 


\subsection{Refactoring lists to Hughes lists}

For functional programmers, lists are a very familiar and versatile data structure. However, if an application requires repeated appends as described at the start of this section their performance becomes an issue. Difference lists provide a similar interface to lists but without this troublesome behaviour. This section will describe a refactoring to convert programs written using normal lists to instead use difference lists. 

\subsubsection{Isomorphic Types} 
This refactoring takes the view that a type consists of some structure and a set of functions that operate on that structure. 

Two types, \textit{A} and \textit{B} are considered isomorphic if there exists a projection function \textit{proj :: A -> B}, an abstraction function \textit{abs :: B -> A}, and a set of pairs where each pair consists of one function that operates over \textit{A} and the function that performs the same operation over \textit{B}. For example, if type \textit{A} had a function \textit{add} and \textit{B} has a function \textit{insert} that both add a new element to the respective collections these two functions should be paired. The refactoring modifies the target function(s) by replacing functions using the old type with the function they are paired with. If functions exist that are not in the set of pairs then the projection and/or abstraction functions are inserted to change the type of that particular branch of the abstract syntax tree.

In the list to Hughes list case the projection function is \texttt{fromList} (because it projects lists into the new type \texttt{DList}) and the abstraction function is \texttt{toList}. What to define the set of pairs as is an interesting problem with multiple "correct" solutions. The \texttt{Data.DList} module exports the following functions~\citep{dlist}.

\begin{center}
\begin{tabular}{| c | c | c |}
  \hline
  \texttt{apply} & \texttt{empty} & \texttt{singleton}\\
  \hline
  \texttt{cons} & \textbf{\texttt{snoc}} & \texttt{append} \\
  \hline
  \texttt{concat} & \texttt{replicate} & \textbf{\texttt{list}}\\ 
  \hline	
  \texttt{head} & \texttt{tail} & \texttt{unfoldr}\\ 
  \hline  
  \texttt{foldr} & \texttt{map} \\
  \hline
\end{tabular}
\end{center}

From this list the two bolded functions\footnote{(\texttt{snoc :: DList a -> a -> DList a}) appends a single element to a list}\footnote{(\texttt{list :: b -> (a -> DList a -> b) -> Dlist a -> b}) is list elimination for dlists} are the only functions without normal list counterparts. The rest of the \texttt{DList} API could be paired with equivalent list functions. However, its not necessarily a good idea for every normal list function to be refactoring to its \texttt{DList} equivalent. As was mentioned certain \texttt{DList} functions are less efficient than the corresponding normal function, the primary purpose of this refactoring is defeated if the refactored code runs slower than the original source.

Fortunately different versions of the refactoring can be made by defining separate sets depending on the behaviour that is desired. For example one set could only include the \texttt{DList} constant time operations (\texttt{append}, \texttt{empty}, and \texttt{cons}) and another set could include all possible pairings. 

\subsubsection{Transforming functions}

The refactoring breaks into three different cases depending on the type of the target function. 

In section~\ref{introSyn} there was a point when the refactoring required the user to tell HaRe where the new synonym should be used because the tool does not know what is the user's intention for the synonym. Many data-driven refactorings need to either make assumptions about what the user's intent is or directly solicit information from them. This refactoring is no different, for any given function there multiple correct definitions. Take for example the function \texttt{insComma} in listing~\ref{insComma}.

\begin{lstlisting}[label=insComma,caption={}]
insComma :: String -> String -> String
insComma s1 s2 = s1 ++ "," ++ s2
\end{lstlisting}  

When refactoring both of the arguments to and the result type of \texttt{insComma} to all become \texttt{DList Char} there are multiple ways to refactor this function. Listing~\ref{commaRef} shows two possibilities, which one should the refactoring produce and why?

To make this decision, the refactoring must infer that because this function is being refactored to use the new type the user wants the new type to be used in as many places as possible. This makes the first definition preferable to the second one because it only converts a single item using \texttt{fromList} and replaces the appends whereas the second example converts the two arguments into lists appends everything together and then converts the result back into a \texttt{DList}

\begin{lstlisting}[label=commaRef,caption={}]
insComma_1 :: DList Char -> DList Char -> DList Char
insComma_1 s1 s2 = s1 `append` fromList (",") `append` s2

insComma_2 :: DList Char -> DList Char -> DList Char
insComma_2 s1 s2 = fromList ((toList s1) ++ (",") ++ (toList s2))
\end{lstlisting}

\subsubsection{Example 1}

The first example covers the case where the result type of a function is refactored to use \texttt{DList} instead of list. Listing~\ref{enumBefore} contains the definition of a simple algebraic data type of a tree, a function (\texttt{enumerate}) that returns an in-order list of all the tree's elements, and a function that prints a tree's enumeration to standard output. 

The refactoring will affect the result type of \texttt{enumerate} so that it returns a \texttt{DList a} instead of the standard list. For this refactoring the set of pairs only needs to contain the two pairs \texttt{((++), append)} and \text{([], empty)}.

\begin{lstlisting}[caption={Definition of enumerate}, label=enumBefore]
data Tree a = Leaf
            | Node (Tree a) a (Tree a)

enumerate :: Tree a -> [a]
enumerate Leaf = []
enumerate (Node left x right) = (enumerate left) ++ [x] ++ (enumerate right)

printEnumTree :: (Show a) => Tree a -> IO ()
printEnumTree tree = let lst = enumerate tree in
  print lst
\end{lstlisting}

Since the result type is the only type of \texttt{enumerate} that is being affected the refactoring needs to modify the function definition so that the right hand side of its definition is of type \texttt{DList a} instead of \texttt{[a]}. It does this by traversing the abstract syntax tree in a top down manner. 


\chapter{Generalising Monads to Applicative}
\label{applicative}

The previous chapter introduced the concept of a functional data refactoring and gave two examples, introducing a type synonym and generalising Maybe to MonadPlus. This chapter will cover another generalising refactoring in more depth, rewriting monadic functions to use applicative functors. 

In their 2008 functional pearl "Applicative programming with effects" Conor McBride and Ross Paterson introduced a new typeclass that they called Idioms but are also known as Applicative Functors~\citep{mcbrideIdioms}. Idioms provide a way to run effectful computations and collect them in some way. They are more expressive than functors but more general than Monads, further work was done in~\citep{arrowsAndIdioms} to prove that Idioms are also less powerful than Arrows.

Applicative functors were implemented in GHC as the typeclass \texttt{Applicative}. An interesting part of the history of GHC is that despite McBride and Paterson proving in their original functional pearl that all monads are also applicative functors, however,  GHC did not actually require instances of monad to also be instances of Applicative until GHC's 7.10.1 release~\citep{ghc7.10Release}. Now that every monad must also be an applicative functor there now exists a large amount of code which could be rewritten using the applicative operators rather than the monadic ones. 

This chapter will discuss the design and implementation of a refactoring which will automatically refactor code written in a monadic style to use the applicative operators instead. Section~\ref{sec:appOverview} is a brief overview of the \texttt{Applicative} typeclass's operators, section~\ref{sec:appProgStyle} will discuss the applicative programming style and, in general, how programs are constructed using the applicative operators, next, section~\ref{sec:appApps} will cover some common applications of this refactoring, section~\ref{sec:appRefact} will specify the refactoring itself, section~\ref{sec:appPrecons} covers the preconditions of the refactoring, finally section~\ref{sec:appVariations} outlines other refactorings that may be used in conjunction with the generalising monads to applicative refactoring and some possible variations of this refactoring. 

\section{The Applicative Typeclass}
\label{sec:appOverview}

The \texttt{Functor} typeclass defines a single function that must be implemented, \texttt{fmap}.

\begin{lstlisting}[frame=tblr]
class Functor f where
	fmap :: (a -> b) -> f a -> f b
\end{lstlisting}

The \texttt{fmap} function allows for a function to be applied to the contents of the Functor f. One could think of the functor as a context and \texttt{fmap} as a function that allows other functions to run within that context. However, what if you wanted to chain together sequences of commands within that context? This is not possible with just functors since \texttt{fmap} does not have the function inside of the functor's context. Sequencing commands will require a more powerful abstraction, applicative functors~\citep{realWorldHaskell}. 

In Haskell applicative functors are implemented in the \texttt{Applicative} typeclass. \texttt{Applicative} typeclass declares two functions, \texttt{pure} and \texttt{(<*>)}. The types of these two functions are shown in listing~\ref{appTypes} where \texttt{f} is the applicative functor. 

\begin{lstlisting}[frame=tblr,label=appTypes,caption={Types of Applicative's minimal complete definition}]
pure :: a -> f a
(<*>) :: f (a -> b) -> f a -> f b
\end{lstlisting}

The \texttt{pure} function is the equivalent of monad's \texttt{return}, it simply lifts a value into the applicative context. The other function \texttt{(<*>)} (which is typically pronounced "applied over" or just "apply"). Apply take in two arguments, both of which are applicative values. The first argument is function within an applicative context from types a to b, and the second argument is of type a. Apply returns a value of type b inside of the same functional context. Apply "extracts" the function from the first argument and the value from the second argument and applies it to the function, all within whatever the applicative context is.

\subsection{Other useful functions}

Though \texttt{pure} and apply are the only two functions that are required to be defined to declare an instance of applicative there are several other useful functions that can either be derived from these two functions or come from other typeclasses which will be briefly covered here. First there are two variations on apply.

\begin{lstlisting}[frame=tblr]
(*>) :: f a -> f b -> f b
(<*) :: f a -> f b -> f a
\end{lstlisting}

These functions sequence actions and still perform the contextual effects of both of their arguments but discard the value of the first and second argument respectively. These functions are used when some operation affects the applicative context but their returned value will not affect the final result of the applicative expression. For example when writing parsers it is common to have to consume some characters from the input without those characters affecting the final result of the parser.

A consequence of the applicative laws is that every applicative's functor instance will satisfy the following~\citep{control.applicative}: 

\begin{lstlisting}[frame=tblr]
f <$> x = pure f <*> x
\end{lstlisting}

The next section will cover how these functions can be used in an applicative style of programming. 

\section{The Applicative Programming Style}
\label{sec:appProgStyle}

In~\citep{mcbrideIdioms} the authors prove that any expression built from the applicative combinators can take the following canonical form:

\begin{lstlisting}[frame=tblr]
pure f <*> is_1 <*> ... <*> is_n
\end{lstlisting}


Where some of the \texttt{is}'s have the form \texttt{pure s} for a pure function \texttt{s}. Due to the rule mentioned at the end of the previous section this canonical form can also be expressed using the infix version of fmap \texttt{(<\$>)}. 

\begin{lstlisting}[frame=tblr]
f <$> is_1 <*> ... <*> is_n
\end{lstlisting}

This is the form that most programs will take when they are refactored from a monadic style. 


 Context-free parsing is a good use case of the applicative type and many examples in this chapter are taken from parsers defined using the parsec library~\citep{parsec}. The first example of the applicative programming style is a function that parses money amounts of the form \texttt{<currency symbol><whole currency amount>.<decimal amount>} e.g. "\$4.59" or "\textsterling64.56".
 
 \begin{lstlisting}[frame=tblr]
 data Currency = Dollar
                          | Pound
                          | Euro
              
data Money = M Currency Integer Integer

parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
 \end{lstlisting}
 
The \texttt{parseMoney} function is in the canonical form as defined by~\citep{mcbrideIdioms}. The pure function \texttt{M} is lifted into the \texttt{CharParser} context and its three arguments are provided by three smaller parsers that handle the currency symbol, the whole amount, and the decimal amount separately. 

The only difference between \texttt{readWhole} and \texttt{readDecimal} is that \texttt{readDecimal} has to consume the decimal point before reading the number. Instead of duplicating that number code let's perform a small refactoring to lift the parsing of the decimal into the \texttt{parseMoney} function which will allow us to reuse the \texttt{readWhole} function.

 \begin{lstlisting}[frame=tblr]
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <* char '.' <*> readWhole
 \end{lstlisting}
 
 Here we can see that the result of parsing the decimal point is discarded because of the use of \texttt{<*} rather than the full apply. All of the variations of apply are left associative so the following definition of \texttt{parseMoney} causes a type error.
 
  \begin{lstlisting}[frame=tblr]
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> char '.' *> readWhole
 \end{lstlisting}
 
This error can be corrected by wrapping "\texttt{char \textquotesingle.\textquotesingle~*> readWhole}" in parenthesis. 
 
The canonical style of applicative functions is not always the most idiomatic way to define things. The following function parses strings surrounded by double quotes.

\begin{lstlisting}[frame=tblr]
parseStr :: CharParser () String 
parseStr = char '"' *> (many1 (noneOf "\"")) <* char '"'
\end{lstlisting}

\texttt{parseStr} does not match the canonical form because no lifted pure function is applied to the rest of the applicative chain. This function could be transformed to canonical form by pre-pending "\texttt{id <\$>}."

The examples covered in this section give a basic introduction to programming in an applicative style. The next section will discuss common applications that are particularly well suited to definition in the applicative style and can be transformed from the monadic style. 

\section{Applications of the Refactoring}
\label{sec:appApps}

There are two things that make a particular application a good candidate for this refactoring. First, and most obviously, the application must be able to be defined using the applicative interface. Finally a good candidate will already have a large corpus of code that is already written in the monadic style. If a particular library already encourages its users to use it using applicative functors rather than monads then there is little work for the refactoring to do.

\subsection{Parsec}
Parser combinator libraries such as parsec~\citep{parsec} provide a simple way of building parsers from predefined smaller parsers (a.k.a. the combinators). The applicative interface is sufficient for defining parsers of context-free languages\footnote{This is mostly true. The applicative interface can parse context-sensitive languages if your grammar is an infinite size~\citep{appContextSens}.}. Despite this much of the code written using Parsec is monadic. A good example of this comes from Real World Haskell~\citep{realWorldHaskell}.

\begin{lstlisting}[frame=tblr]
csvFile :: GenParser Char st [[String]]
csvFile = 
    do result <- many line
       eof
       return result
\end{lstlisting}

This can be very simply rewritten using the applicative like so:

\begin{lstlisting}[frame=tblr]
csvFile :: GenParser Char st [[String]]
csvFile = many line <* eof 
\end{lstlisting}      
 
 Shorter code is not always better however, in this case,  I would argue that the applicative style is easy to clearer. The parser can be read left to right many lines are followed by the end of file.
 
\subsection{Yesod}
Another possible application of this refactoring applies to parts of code used to define Yesod webservers~\citep{yesod}. The preferred way to handle the creation and processing of web forms is using the applicative interface~\citep{yesodBook}. Yesod doesn't force forms to be handled applicatively because a monad instance is provided as well but it is the \textit{idiomatic} way to handle web forms. This refactoring would allow people to write in a monadic style and then refactor their code to fit the standard.

\subsection{Other applications}
\comment{Complete section conclusion here}
  

\section{Refactoring Monadic Programs to Applicative}
\label{sec:appRefact}
This section will cover the mechanics of refactoring monadic code to the applicative style. Many of these examples are taken from the parser for money amounts and a JSON parser. The full source of these parse can be found in~\citep{moneyParse} and~\citep{jsonParser}.

Take for example the following parser that parses strings that begin and end with double quotes.

\begin{lstlisting}[frame=tlrb]
parseStr :: CharParser() String
parseStr = do
	char '"'
	str <- many1 (noneOf "\"")
	char '"'
	return str
\end{lstlisting}

This parser first consumes a double quote (\texttt{char \textquotesingle"\textquotesingle}) then parses at least one other character other than double quotes and assigns those characters to the variable named \texttt{str}\footnote{This line is composed of two parser combinators, \texttt{many1}, and \texttt{noneOf}. \texttt{many1} takes another parser as its argument and applies it one or more times returning a list of the results. \texttt{noneOf} takes in a list of characters and succeeds if the current character is not in the provided list. Then the character is returned.}, finally the closing quote is consumed and \texttt{str} is returned. This particular function can be rewritten in an applicative style like so:

\begin{lstlisting}[frame=tlrb]
parseStr :: CharParser() String
parseStr = char '"' *> (many1 (noneOf "\"")) <* char '"'
\end{lstlisting}

The refactoring goes through the monadic version of the function line by line and composes computations with the applicative operators. In this case the first line of the \texttt{do} block does not affect the final result so it is followed by the \texttt{*>} which performs the action on the left hand side of the operator but discards that value. The next line's value is assigned to the variable str which is returned by the function so this means that on both sides of this computation the operator that composes it with its neighbours will have to "point" to it as well\footnote{This means \texttt{<*>} could occur on both sides, \texttt{*>} on the left, or \texttt{<*} on the right of the computation}. To determine which operator needs to be used on the right of the call to \texttt{many1} the refactoring needs to look at the next line and see if it also contributes a value to the final result. If it does then it and \texttt{many1} will be composed by the \texttt{<*>} operator, but since \texttt{char} doesn't, the operator between the call to \texttt{many1} and the second call to \texttt{char} becomes \texttt{<*} which discards the value \texttt{char} returns.

This is a fairly simple function to convert to applicative style. Let's look at another example that adds in more complexity by having multiple computations that contribute to the final value of the function. This function comes from the money parser that was used in~\ref{sec:appProgStyle} as well. 
\pagebreak
\begin{lstlisting}[frame=tlrb]
parseMoney :: CharParser () Money
parseMoney = do
   currency <- parseCurrency 
   whole <- many1 digit
   decimal <- (option "0" (do { 
                           char '.';
                           d <- many1 digit;
                           return d}))
   return $ M currency (read whole) (read decimal)
\end{lstlisting}

The \texttt{parseMoney} function parses text into the \texttt{Money} data type. It works by first consuming the currency symbol and getting the appropriate \texttt{Currency} type from that. Then the whole money amount is read from one or more digits. Finally an "option" parser attempts to match a decimal point followed by one or more digits. If that fails then the decimal amount is zero. These three values are then combined into type \texttt{Money} which is returned. \texttt{parseMoney} can be rewritten in an applicative style like so\footnote{The where clause in this example has been included for formatting and readability and would not be generated automatically.}:

\begin{lstlisting}[frame=tlrb]
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
          where readWhole = read <$> many1 digit
                  readDecimal = read <$> option "0" (do { 
                           char '.';
                           d <- many1 digit;
                           return d}))
\end{lstlisting}

On the left hand side of the chain of applicative actions there is a call to a pure operation, in this case the constructor \texttt{M}. Any pure computations that "collect" the values returned from applicative computations will appear on the left of the chain of operations. The pure computations are composed with applicative computations with the \texttt{<\$>} operator which lifts the computation into the applicative context. The three computations are composed together with the \texttt{<*>} operator because they all contribute to the final result of \texttt{parseMoney}.

The chains of applicative computations can get more complicated. The following snippet of code parses a single in a JSON object which consists of a string key and a value which can be any valid JSON value, separated by a colon, and stores the key and value in a tuple. 

\begin{lstlisting}[frame=tblr]
objEntry = (,) <$> (spaces *> parseStr <* spaces <* char ':' <* spaces) <*> (parseJVal <* spaces)
\end{lstlisting}

When there are a large amount of computations that do not affect the final value of the function as a whole there can be multiple valid ways the chain can be structured. The \texttt{objEntry} function can be defined in several different ways as shown below.

\begin{lstlisting}[frame=tblr]
objEntry = (,) <$> (spaces *> parseStr <* spaces <* char ':') <*> (spaces *> parseJVal <* spaces)

objEntry = (,) <$> (spaces *> parseStr) <*> (spaces *> char ':' *> spaces *> parseJVal <* spaces)
\end{lstlisting}

Both of the above versions of \texttt{objEntry} are equivalent to the first version. The automated refactoring will produce the first version of \texttt{objEntry}. The refactoring in general will produce applicative chains according to the following rules. Both sides of the apply operator will be parenthesised statements. After the first value producing operation every side effect causing operation will be composed with \texttt{(<*)}. In general the produced applicative chain will take the following form.

\begin{lstlisting}[frame=tblr]
f = pf <$> (is_1 *> ... is_n *> vs_1 <* js_1 ... <* js_n) <*> (vs_2 <* ks_1 ... <* ks_n) ... <*> (vs_n ...)
\end{lstlisting}

The \texttt{vs} are functions that return values to be passed to the pure function \texttt{pf} all other functions just run within the applicative context without affecting the returned value of \texttt{f}.

\section{Preconditions of the Refactoring (When is a Monad actually a Monad?)}
\label{sec:appPrecons}

Many functional refactorings have non-trivial preconditions that must hold before the refactoring can be applied~\citep{refacTools}. Fortunately this refactoring only has a single fairly simple precondition, the function to be refactored must be definable with the applicative interface not just the monadic interface. What does this mean exactly? Where is the line between applicative and monadic? Let's start by looking at the type signatures of the bind and apply functions.

\begin{lstlisting}[frame=tblr]
(<*>) :: Applicative f => f (a -> b) -> f a -> f b

(>>=) :: Monad m => m a -> (a -> m b) -> m b
\end{lstlisting}  

One thing to keep in mind is that these two functions' arguments are in opposite order. The key difference becomes clearer when examining the type of apply when its arguments are flipped.

\begin{lstlisting}[frame=tblr]
flip (<*>) :: Applicative f => f a -> f (a -> b) -> f b
\end{lstlisting}

The only difference between this version of apply and bind is in the second argument. Bind takes in a function that takes in a value of type \texttt{a} and returns an \texttt{m b} whereas apply receives an applicative functor that contains a function from \texttt{a} to \texttt{b}. This means that within a monadic context bind allows access to the pure value contained in the monad while all of the arguments to apply are contained within the applicative functor.

What does this mean in practice? For example, the following functions are taken from a StackOverflow answer by Conor McBride~\citep{soApp}.

\begin{lstlisting}[frame=tblr]
iffy :: Applicative a => a Bool -> a x -> a x -> a x
iffy ab at af = pure cond <*> ab <*> at <*> af where
  cond b t f = if b then t else f

miffy :: Monad m => m Bool -> m x -> m x -> m x
miffy mb mt mf = do
  b <- mb
  if b then mt else mf
\end{lstlisting} 

Both of these functions attempt to implement an if statement, \texttt{iffy} does it with \texttt{pure} and \texttt{(<*>)} whereas \texttt{miffy} uses the monadic functions. Both functions' first argument contains a boolean within its computational context. Depending on the value of this boolean the second or third argument is then returned. Though both of these functions will return the same value when given the same arguments, the effects within the context will be very different. When \texttt{iffy} is run all of the contextual effects will be run regardless of which value is returned.

If a value retrieved from the monadic context is used in a right hand side expression before the return statement then that function cannot be refactored to use the applicative interface without changing the contextual effects of the function. 

\section{Variations and Related Refactorings}
\label{sec:appVariations}

This section will discuss related refactorings and variations on the generalise monad to applicative refactoring that may be useful. Related refactorings help transform code so that it can pass the preconditions. In this case it may be possible to extract monadic code into another function making the top level function definable with the applicative interface. Variations on refactorings slightly change the behavior of the refactoring. This section will present two variations, one which will inline small do blocks of monadic code automatically, another will
recursively refactor do blocks that may occur inside of higher level statements.

\subsection{Extract monadic code}
\label{subSec:extract}
Say someone wanted to refactor the following code to use the applicative interface rather than the monadic one it currently uses.

\begin{lstlisting}[frame=tblr]
f = do
	x <- getX
	b <- getB
	y <- if b then getY1 else getY2
	log y
	return (x,y)	
\end{lstlisting}

This code will not pass the precondition because both \texttt{b} and \texttt{y} are used on the right as well as the left hand side of the equation. However, lines three through five don't really affect the rest of the function so they could be refactored into their own function then f could be rewritten applicatively.

\begin{lstlisting}[frame=tblr]
f = (,) <$> getX <*> g

g = do
	b <- getB
	y <- if b then getY1 else getY2
	log y
	return y
\end{lstlisting}

\subsection{Inline do blocks}
Instead of extracting an entire function as in subsection~\ref{subSec:extract} a developer may prefer to just inline a do block. This is useful if the monadic section is fairly small.

\begin{lstlisting}[frame=tblr]
f = do
	x <- result1
	y <- result2
	z <- result3
	log z
	return (x,y)
\end{lstlisting}
\larger[5]
\[\Rightarrow\]
\normalsize
\begin{lstlisting}[frame=tblr]
f = (,) <*> result1 <*> (result2 <* do{z <- result3; log z})
\end{lstlisting}

Normally the variable \texttt{z} would prevent the function from being refactored. Introducing the small do block allows for a simple readable applicative function to be produced. 

It is worth noting that if the variable \texttt{z} was also included in the output of the function the do block inlining would still work with a slight modification.

\begin{lstlisting}[frame=tblr]
f = (,) <*> result1 <*> result2 <*> do{z <- result3; log z; return z}
\end{lstlisting}

\subsection{Refactor Inner \texttt{do} blocks}
If we take another look at the \texttt{parseMoney} function and it's applicative counterpart the default behaviour of the refactoring preserves the inner do block passed to the \texttt{option} parser.

\begin{lstlisting}[frame=tlrb]
parseMoney :: CharParser () Money
parseMoney = do
   currency <- parseCurrency 
   whole <- many1 digit
   decimal <- (option "0" (do { 
                           char '.';
                           d <- many1 digit;
                           return d}))
   return $ M currency (read whole) (read decimal)
   
parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
          where readWhole = read <$> many1 digit
                  readDecimal = read <$> option "0" (do { 
                           						char '.';
                          						d <- many1 digit;
                           						return d}))
\end{lstlisting}

It is perfectly possible to refactor this inner do block to the applicative style as well.

\begin{lstlisting}[frame=tblr]

parseMoney :: CharParser () Money
parseMoney = M <$> parseCurrency <*> readWhole <*> readDecimal
          where readWhole = read <$> many1 digit
                  readDecimal = read <$> option "0" (char '.' *> many1 digit)
\end{lstlisting}


\chapter{Introducing Effectful Abstractions}
Up to this point the data refactorings that have been discussed are changing abstractions that already existed in the source code. This chapter will explore refactorings that introduce effectful abstractions into pure code. In particular this chapter will focus on introducing monads and applicatives into pure code.

The \texttt{Identity} monad is the monad the does not embody any computation strategy~\citep{identityMonad}. This means that any pure Haskell function could be refactored to be within the Identity monad. This refactoring can take in a set of functions and produce a corresponding set of functions with a monadic type. Take for example this definition of the Fibonacci numbers.

\begin{lstlisting}[frame=tblr]
fib :: Int -> [Int]
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
\end{lstlisting} 

This can be refactored to the Identity monad like so:\pagebreak

\begin{lstlisting}[frame=tblr]
fib :: Int -> Identity [Int]
fib 0 = return 0
fib 1 = return 1
fib n = do
	x <- fib (n-1)
	y <- fib (n-2)
	return (x + y)
\end{lstlisting}

The new code could then be rewritten very easily (just by changing the type signature) to be within another monad. This allows for a developer to quickly create programs that can take advantage of monadic features such as IO or state. The monadic code could also be generalised to use applicatives with the refactoring detailed in chapter~\ref{applicative} but I also hope to develop a more straightforward way to introduce applicatives for this chapter.

Finally another abstraction that would seem to have quite a bit of potential for automatic introduction is the \texttt{Arrow} typeclass. Arrows were originally introduced as a more general abstraction to monads in~\citep{genMonadsArrows}. The full relationship between arrows, monads, and applicative functors was more fully described in~\citep{arrowsAndIdioms}. Given the relationship between these three typeclasses I believe it will be worth exploring introducing arrows as well. A possible outline of this chapter would be:

\begin{enumerate}
\item Introducing the Applicative style
\item Automated Monadification
\item Introduction to Arrows
\item Discussion of Arrows, Applicative, and Monads
\item Refactoring to Arrows
\end{enumerate} 

\if(FALSE)
This chapter will cover the refactoring of pure code to become monadic. This chapter will be structured much like chapter~\ref{applicative} with sections covering the motivation behind the refactoring, examples of the refactoring, the preconditions that must hold before applying the refactoring, and variations and related refactorings. 

The \texttt{Identity} monad is the monad the does not embody any computation strategy~\citep{identityMonad}. This means that any pure Haskell function could be refactored to be within the Identity monad. This refactoring can take in a set of functions and produce a corresponding set of functions with a monadic type. Take for example this definition of the Fibonacci numbers.

\begin{lstlisting}[frame=tblr]
fib :: Int -> [Int]
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
\end{lstlisting} 

This can be refactored to the Identity monad like so:

\begin{lstlisting}[frame=tblr]
fib :: Int -> Identity [Int]
fib 0 = return 0
fib 1 = return 1
fib n = do
	x <- fib (n-1)
	y <- fib (n-2)
	return (x + y)
\end{lstlisting}

The new code could then be rewritten very easily (just by changing the type signature) to be within another monad. This allows for a developer to quickly create programs that can take advantage of monadic features such as IO or state.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


This chapter will cover the refactoring pure code to be monadic instead. Take for example the following standard definition of mergesort.

\begin{lstlisting}[frame=tblr]
mergesort :: (Ord a) => [a] -> [a]
mergesort [] = []
mergesort [x] = [x]
mergesort xs = merge (mergesort fst) (mergesort snd)
  where fst = take (length xs `div` 2) xs
        snd = drop (length xs `div` 2) xs

merge :: (Ord a) => [a] -> [a] -> [a]
merge xs [] = xs
merge [] ys = ys
merge (x:xs) (y:ys)
   | (x <= y) = x:(merge xs (y:ys))
   | otherwise = y:(merge (x:xs) ys)
\end{lstlisting}

Say that one wanted to refactor this code to become monadic. There are two different ways this could be interpreted, either both \texttt{mergesort} and \texttt{merge} are rewritten to be monadic, or just \texttt{mergesort} could be refactored. \texttt{Merge} cannot be refactored to be monadic without also changing \texttt{mergesort} because \texttt{mergesort} is dependent on the result of calls to \texttt{merge}. If \texttt{merge}'s result is monadic any client functions will also have to be monadic because there is no way to remove the value from a monad. 

Refactoring \texttt{mergesort} to be monadic without refactoring \texttt{merge} produces the following code.

\begin{lstlisting}[frame=tblr]
mergesort :: (Ord a) => [a] -> Identity [a]
mergesort [] = return []
mergesort [x] = return [x]
mergesort xs = do
  x <- mergesort fst
  y <- mergesort snd
  return $ merge x y
    where fst = take (length xs `div` 2) xs
             snd = drop (length xs `div` 2) xs
\end{lstlisting}  

Any program can be refactored into the \texttt{Identity} monad as this monad does not express any computational strategy~\citep{identityMonad}. Here we can see that in cases where pure values would have been returned from \texttt{mergesort} pre-refactoring have now been prepended with \texttt{return}, this lifts these values into the \texttt{Identity} monad. The recursive calls to \texttt{mergesort} now need to be bound to variables in the do block which allow the pure results to be passed to calls to the still pure \texttt{merge}.

Instead of refactoring just \texttt{mergesort} refactoring \texttt{merge} as well would produce slightly different code.

\begin{lstlisting}[frame = tblr]
merge :: (Ord a) => [a] -> [a] -> Identity [a]
merge xs [] = return xs
merge [] ys = return ys
merge (x:xs) (y:ys)
   | (x <= y) = do
       ls <- merge xs (y:ys)
       return (x:ls)
   | otherwise = do
       ls <- merge (x:xs) ys
       return (y:ls)

mergesort :: (Ord a) => [a] -> Identity [a]
mergesort [] = return []
mergesort [x] = return [x]
mergesort xs = do
  x <- mergesort fst
  y <- mergesort snd
  merge x y
    where fst = take (length xs `div` 2) xs
            snd = drop (length xs `div` 2) xs
\end{lstlisting}

The above example's definition of \texttt{mergesort} is very similar to the previous definition. The only difference is that previously the call to \texttt{merge} was a pure value so it needed to be lifted into the monad by \texttt{return}, in this version since merge was also refactored into the monad it can just sit at the end of mergesort.

\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related work}

There are several bodies of literature that are related to my thesis work. Other functional refactoring tools such as Wrangler~\citep{wrangler} are of obvious interest. There is also the code smell tool for Haskell HLint~\citep{hlint}.

Another interesting project is the Type-and-Transform system developed at the University of Utrecht~\citep{typeAndTransform}. Which is a system for performing semantics preserving type changing transformations for the simply typed lambda calculus, and the polymorphic lambda calculus.

In general this chapter will briefly discuss the research done in two primary areas:

\begin{enumerate}
\item Refactoring and code smell tools for functional programming languages
\item Type changing program transformations 
\end{enumerate}

\section{Program transformation and Refactoring}

This section covers some of the relevant previous work done in the fields of refactoring and program transformation. It should be said that there is much more work done in program transformation than is discussed here. More general discussion about the field of program transformaton in general can be found in~\citep{visserSurvey,transformationIntro}.

\subsection{Type and transform systems}

Work on a theoretical type changing program transformation system is being done currently at Utrecht University. They have proposed a type changing program transformation system known as a "type-and-transform" system~\citep{typeAndTransform}. This type-and-transform system can transform standard Haskell lists to Hughes' lists and perform stream fusion~\citep{typeAndTransform}. The type-and-transform system was expanded in~\citep{typeAndTransformPatterns} to properly transform Haskel lists to the $Seq~a$ type found in the \textit{Data.Sequence} module. At this point the type-and-transform system only works on a subset of the Haskell language~\citep{typeAndTransform}

\subsection{Automatic Monadification}\label{erwigMonad}

Another type of program transformation that many people are interested in involves the "monadification" of code. Monads are necessary for Haskell code to perform any operation that would typically cause side-effects (I/O, state, etc.). This means that a system that can transform code without monads to code with monads, would be highly desirable to the Haskell community.

Erwig and Ran have proposed just such a system in~\citep{monadification}. Their system can be broken down into three steps. \textit{Navigating} involves finding the expression that will be returned by the function. Next \textit{binding} identifies recursive calls and replace the with variable binding. Finally the return is applied to the expression \textit{navigating} identified in the \textit{wrapping} step.

\subsubsection{Issues with Erwig and Ran's solution}

There are a few issue with the monadification solution proposed by~\citep{monadification}. There several different styles of monadification and this algorithm only supports one of them~\citep{clausMonadResponse}. Five different styles of monadification have been outlined in~\citep{monadSurvey}, and before implementing an automatic monadification solution it should be determined which style of monadification would be the most helpful to the Haskell community.

\subsection{Functional language refactoring tools}

Refactoring is a subtopic of standard program transformation in that the goal of refactoring is to leave program behavior unchanged~\citep{fowler}. There are quite a few functional language refactoring tools that already exist for Haskell and other functional languages. 

\subsubsection{Erlang}

Erlang is another functional programming language that puts a heavy emphasis on fault tolerance, scalability, and concurrency. In contrast to Haskell, Erlang is not statically typed.
 
Several projects exist to improve the quality of Erlang code. RefactErl is a source code analysis tool that also supports a few refactorings~\citep{refactErlWiki}. Tidier is an automated Erlang refactorer. Unlike many of the tools discussed in this paper Tidier runs without user input, cleaning up Erlang source code automatically~\citep{erlangTidier}.

Wrangler is another refactoring tool for Erlang made by Huiqing Li and Simon Thompson here at the University of Kent~\citep{refacTools}. In addition to refactorings Wrangler also provides "decision support tools" that detect code smells and highlight other issues within an Erlang project. Wrangler is also user extensible, it supports a domain-specific language allowing users to script their own complex refactorings~\citep{refacTools}. Wrangler has been integrated into the Emacs and Eclipse IDEs.

\subsubsection{Haskell}

Improving the quality of Haskell code is the focus of several projects. HLint for example provides suggestions for how Haskell code could be improved but does not do any actual transformations~\citep{hlint}.

Another Haskell program transformation project is HERMIT. Instead of performing transformations on Haskell source code HERMIT targets the Core language~\citep{hermit}. Core is the intermediate language that Haskell source code is transformed to after desugaring~\citep{ghcDesign}. HERMIT uses GHC plugins to perform program transformations as compiler optimisation passes. 

Finally there is the Haskell Refactorer (HaRe). Since HaRe is where I will be implementing my research I have dedicated an entire chapter (chapter ~\ref{hareChapter}) to describing its implementation. 

\comment{\textbf{Add section about ML languages.}}

\chapter{Conclusion}
Summarise my contribution here. The main contributions of my thesis are the development of type changing refactorings for GHC. With a particular emphasis on changing the abstractions that programs use.

\bibliography{main}

% This index section is optional, use cleardoublepage and phantomsection to make the links work in your contents page. Uses makeidx package.
\cleardoublepage
\phantomsection
\label{index}
\printindex

\end{document}
